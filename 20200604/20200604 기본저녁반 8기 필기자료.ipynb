{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#tensorflow_datasets\" data-toc-modified-id=\"tensorflow_datasets-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>tensorflow_datasets</a></span></li><li><span><a href=\"#Deep-Learning-Regression\" data-toc-modified-id=\"Deep-Learning-Regression-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Deep Learning Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#preprocessing\" data-toc-modified-id=\"preprocessing-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#na-여부\" data-toc-modified-id=\"na-여부-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>na 여부</a></span></li><li><span><a href=\"#Encoding-여부\" data-toc-modified-id=\"Encoding-여부-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Encoding 여부</a></span></li><li><span><a href=\"#normalization-여부\" data-toc-modified-id=\"normalization-여부-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>normalization 여부</a></span></li></ul></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sequential\" data-toc-modified-id=\"Sequential-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Sequential</a></span></li><li><span><a href=\"#create_model\" data-toc-modified-id=\"create_model-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>create_model</a></span></li></ul></li><li><span><a href=\"#fit\" data-toc-modified-id=\"fit-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>fit</a></span><ul class=\"toc-item\"><li><span><a href=\"#문자열-있을-때\" data-toc-modified-id=\"문자열-있을-때-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>문자열 있을 때</a></span></li><li><span><a href=\"#na-있을-때\" data-toc-modified-id=\"na-있을-때-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>na 있을 때</a></span></li><li><span><a href=\"#데이터-정제-후\" data-toc-modified-id=\"데이터-정제-후-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>데이터 정제 후</a></span></li><li><span><a href=\"#Encoding-했을-때\" data-toc-modified-id=\"Encoding-했을-때-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>Encoding 했을 때</a></span></li><li><span><a href=\"#normalization-후\" data-toc-modified-id=\"normalization-후-2.3.5\"><span class=\"toc-item-num\">2.3.5&nbsp;&nbsp;</span>normalization 후</a></span></li></ul></li><li><span><a href=\"#history\" data-toc-modified-id=\"history-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>history</a></span></li><li><span><a href=\"#callback\" data-toc-modified-id=\"callback-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>callback</a></span><ul class=\"toc-item\"><li><span><a href=\"#MyCallback\" data-toc-modified-id=\"MyCallback-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>MyCallback</a></span></li><li><span><a href=\"#EarlyStopping\" data-toc-modified-id=\"EarlyStopping-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>EarlyStopping</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [AI 오픈 이노베이션] 기본 저녁반<a class=\"tocSkip\">\n",
    "GitHub link: [https://github.com/realblack0/8th_ai_lecture_fundamental](https://github.com/realblack0/8th_ai_lecture_fundamental)  \n",
    "E-Mail: realblack0@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow의 연습용 데이터를 제공하는 라이브러리이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'aeslc',\n",
       " 'aflw2k3d',\n",
       " 'amazon_us_reviews',\n",
       " 'arc',\n",
       " 'bair_robot_pushing_small',\n",
       " 'beans',\n",
       " 'big_patent',\n",
       " 'bigearthnet',\n",
       " 'billsum',\n",
       " 'binarized_mnist',\n",
       " 'binary_alpha_digits',\n",
       " 'c4',\n",
       " 'caltech101',\n",
       " 'caltech_birds2010',\n",
       " 'caltech_birds2011',\n",
       " 'cars196',\n",
       " 'cassava',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'cfq',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar10_1',\n",
       " 'cifar10_corrupted',\n",
       " 'citrus_leaves',\n",
       " 'cityscapes',\n",
       " 'civil_comments',\n",
       " 'clevr',\n",
       " 'cmaterdb',\n",
       " 'cnn_dailymail',\n",
       " 'coco',\n",
       " 'coil100',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'cos_e',\n",
       " 'curated_breast_imaging_ddsm',\n",
       " 'cycle_gan',\n",
       " 'deep_weeds',\n",
       " 'definite_pronoun_resolution',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'div2k',\n",
       " 'dmlab',\n",
       " 'downsampled_imagenet',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'duke_ultrasound',\n",
       " 'dummy_dataset_shared_generator',\n",
       " 'dummy_mnist',\n",
       " 'emnist',\n",
       " 'eraser_multi_rc',\n",
       " 'esnli',\n",
       " 'eurosat',\n",
       " 'fashion_mnist',\n",
       " 'flic',\n",
       " 'flores',\n",
       " 'food101',\n",
       " 'gap',\n",
       " 'gigaword',\n",
       " 'glue',\n",
       " 'groove',\n",
       " 'higgs',\n",
       " 'horses_or_humans',\n",
       " 'i_naturalist2017',\n",
       " 'image_label_folder',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imagenet_resized',\n",
       " 'imagenette',\n",
       " 'imagewang',\n",
       " 'imdb_reviews',\n",
       " 'iris',\n",
       " 'kitti',\n",
       " 'kmnist',\n",
       " 'lfw',\n",
       " 'librispeech',\n",
       " 'librispeech_lm',\n",
       " 'libritts',\n",
       " 'lm1b',\n",
       " 'lost_and_found',\n",
       " 'lsun',\n",
       " 'malaria',\n",
       " 'math_dataset',\n",
       " 'mnist',\n",
       " 'mnist_corrupted',\n",
       " 'movie_rationales',\n",
       " 'moving_mnist',\n",
       " 'multi_news',\n",
       " 'multi_nli',\n",
       " 'multi_nli_mismatch',\n",
       " 'natural_questions',\n",
       " 'newsroom',\n",
       " 'nsynth',\n",
       " 'omniglot',\n",
       " 'open_images_v4',\n",
       " 'opinosis',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'patch_camelyon',\n",
       " 'pet_finder',\n",
       " 'places365_small',\n",
       " 'plant_leaves',\n",
       " 'plant_village',\n",
       " 'plantae_k',\n",
       " 'qa4mre',\n",
       " 'quickdraw_bitmap',\n",
       " 'reddit_tifu',\n",
       " 'resisc45',\n",
       " 'rock_paper_scissors',\n",
       " 'rock_you',\n",
       " 'scan',\n",
       " 'scene_parse150',\n",
       " 'scicite',\n",
       " 'scientific_papers',\n",
       " 'shapes3d',\n",
       " 'smallnorb',\n",
       " 'snli',\n",
       " 'so2sat',\n",
       " 'speech_commands',\n",
       " 'squad',\n",
       " 'stanford_dogs',\n",
       " 'stanford_online_products',\n",
       " 'starcraft_video',\n",
       " 'sun397',\n",
       " 'super_glue',\n",
       " 'svhn_cropped',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tf_flowers',\n",
       " 'the300w_lp',\n",
       " 'tiny_shakespeare',\n",
       " 'titanic',\n",
       " 'trivia_qa',\n",
       " 'uc_merced',\n",
       " 'ucf101',\n",
       " 'vgg_face2',\n",
       " 'visual_domain_decathlon',\n",
       " 'voc',\n",
       " 'wider_face',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wmt14_translate',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_t2t_translate',\n",
       " 'wmt_translate',\n",
       " 'xnli',\n",
       " 'xsum',\n",
       " 'yelp_polarity_reviews']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.list_builders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelInterleaveDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelInterleaveDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "data = tfds.load('mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n",
       " 'train': <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary이다.\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op ParallelInterleaveDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelInterleaveDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelInterleaveDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelInterleaveDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "# as_supervised 옵션의 차이를 비교해보자.\n",
    "a = tfds.load('mnist', as_supervised=False)\n",
    "b = tfds.load('mnist', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n",
       " 'train': <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>,\n",
       " 'train': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a와 비슷해 보인다.\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op ParallelInterleaveDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelInterleaveDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "# with_info는 데이터 설명을 함께 return한다.\n",
    "a, b = tfds.load('mnist', as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>,\n",
       " 'train': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫번째 return은 데이터이다.\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    version=3.0.0,\n",
       "    description='The MNIST database of handwritten digits.',\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    total_num_examples=70000,\n",
       "    splits={\n",
       "        'test': 10000,\n",
       "        'train': 60000,\n",
       "    },\n",
       "    supervised_keys=('image', 'label'),\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두번째 return은 데이터 설명이다.\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MaxIntraOpParallelismDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrivateThreadPoolDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op IteratorGetNextSync in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "{'image': <tf.Tensor: id=2036, shape=(10, 28, 28, 1), dtype=uint8, numpy=\n",
      "array([[[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]],\n",
      "\n",
      "\n",
      "       [[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]],\n",
      "\n",
      "\n",
      "       [[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]],\n",
      "\n",
      "\n",
      "       [[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]],\n",
      "\n",
      "\n",
      "       [[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]]], dtype=uint8)>, 'label': <tf.Tensor: id=2037, shape=(10,), dtype=int64, numpy=array([4, 1, 0, 7, 8, 1, 2, 7, 1, 6], dtype=int64)>}\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "for i in data['train'].batch(10):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <tf.Tensor: id=2062, shape=(28, 28, 1), dtype=uint8, numpy=\n",
      "array([[[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [ 84],\n",
      "        [254],\n",
      "        [101],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [174],\n",
      "        [253],\n",
      "        [119],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [ 31],\n",
      "        [247],\n",
      "        [202],\n",
      "        [ 29],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  1],\n",
      "        [  1],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [141],\n",
      "        [253],\n",
      "        [168],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [ 66],\n",
      "        [208],\n",
      "        [ 56],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [186],\n",
      "        [253],\n",
      "        [120],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [ 57],\n",
      "        [253],\n",
      "        [119],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [ 28],\n",
      "        [249],\n",
      "        [240],\n",
      "        [ 25],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [ 34],\n",
      "        [253],\n",
      "        [119],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [109],\n",
      "        [254],\n",
      "        [197],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [ 53],\n",
      "        [253],\n",
      "        [119],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [135],\n",
      "        [254],\n",
      "        [133],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [133],\n",
      "        [254],\n",
      "        [119],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [ 27],\n",
      "        [240],\n",
      "        [255],\n",
      "        [ 35],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  7],\n",
      "        [235],\n",
      "        [253],\n",
      "        [208],\n",
      "        [151],\n",
      "        [169],\n",
      "        [215],\n",
      "        [253],\n",
      "        [206],\n",
      "        [  2],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [ 97],\n",
      "        [253],\n",
      "        [253],\n",
      "        [253],\n",
      "        [254],\n",
      "        [253],\n",
      "        [253],\n",
      "        [253],\n",
      "        [ 86],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [150],\n",
      "        [244],\n",
      "        [145],\n",
      "        [119],\n",
      "        [101],\n",
      "        [ 82],\n",
      "        [253],\n",
      "        [253],\n",
      "        [ 14],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [ 84],\n",
      "        [254],\n",
      "        [172],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [174],\n",
      "        [253],\n",
      "        [119],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [237],\n",
      "        [252],\n",
      "        [ 56],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [ 50],\n",
      "        [241],\n",
      "        [182],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [187],\n",
      "        [254],\n",
      "        [249],\n",
      "        [105],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [186],\n",
      "        [253],\n",
      "        [206],\n",
      "        [ 21],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [227],\n",
      "        [242],\n",
      "        [ 32],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [236],\n",
      "        [219],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]],\n",
      "\n",
      "       [[  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0],\n",
      "        [  0]]], dtype=uint8)>, 'label': <tf.Tensor: id=2063, shape=(), dtype=int64, numpy=4>}\n"
     ]
    }
   ],
   "source": [
    "for i in data['train'].take(1):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op MaxIntraOpParallelismDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrivateThreadPoolDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "# for 문에 쓸 수 있으면 iterator로 만들 수 있다.\n",
    "x = iter(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <tf.Tensor: id=2064, shape=(28, 28, 1), dtype=uint8, numpy=\n",
       " array([[[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [ 84],\n",
       "         [254],\n",
       "         [101],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [174],\n",
       "         [253],\n",
       "         [119],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [ 31],\n",
       "         [247],\n",
       "         [202],\n",
       "         [ 29],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  1],\n",
       "         [  1],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [141],\n",
       "         [253],\n",
       "         [168],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [ 66],\n",
       "         [208],\n",
       "         [ 56],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [186],\n",
       "         [253],\n",
       "         [120],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [ 57],\n",
       "         [253],\n",
       "         [119],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [ 28],\n",
       "         [249],\n",
       "         [240],\n",
       "         [ 25],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [ 34],\n",
       "         [253],\n",
       "         [119],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [109],\n",
       "         [254],\n",
       "         [197],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [ 53],\n",
       "         [253],\n",
       "         [119],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [135],\n",
       "         [254],\n",
       "         [133],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [133],\n",
       "         [254],\n",
       "         [119],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [ 27],\n",
       "         [240],\n",
       "         [255],\n",
       "         [ 35],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  7],\n",
       "         [235],\n",
       "         [253],\n",
       "         [208],\n",
       "         [151],\n",
       "         [169],\n",
       "         [215],\n",
       "         [253],\n",
       "         [206],\n",
       "         [  2],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [ 97],\n",
       "         [253],\n",
       "         [253],\n",
       "         [253],\n",
       "         [254],\n",
       "         [253],\n",
       "         [253],\n",
       "         [253],\n",
       "         [ 86],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [150],\n",
       "         [244],\n",
       "         [145],\n",
       "         [119],\n",
       "         [101],\n",
       "         [ 82],\n",
       "         [253],\n",
       "         [253],\n",
       "         [ 14],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [ 84],\n",
       "         [254],\n",
       "         [172],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [174],\n",
       "         [253],\n",
       "         [119],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [237],\n",
       "         [252],\n",
       "         [ 56],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [ 50],\n",
       "         [241],\n",
       "         [182],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [187],\n",
       "         [254],\n",
       "         [249],\n",
       "         [105],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [186],\n",
       "         [253],\n",
       "         [206],\n",
       "         [ 21],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [227],\n",
       "         [242],\n",
       "         [ 32],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [236],\n",
       "         [219],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]]], dtype=uint8)>,\n",
       " 'label': <tf.Tensor: id=2065, shape=(), dtype=int64, numpy=4>}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterator는 next로 값 뽑을 수 있다.\n",
    "next(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 수업은 [Tensorflow 공식 튜토리얼](https://www.tensorflow.org/tutorials/keras/regression?hl=ko)을 참조하여 진행하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# GPU가 없으면 리소스만 더 먹는다.\n",
    "# tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JINHYO\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = sns.load_dataset('mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\JINHYO\\\\.keras\\\\datasets\\\\auto-mpg.data'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 웹에서 파일 가져오는 방법(딥러닝과 무관)\n",
    "# 이미 다운로드 받았으면 다시 다운받지 않고 local에 있는 애 불러온다.(cache)\n",
    "# 파일을 다운로드 받은 위치를 return한다.\n",
    "tf.keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로우에서도, 정형데이터를 다룰 때는 pandas를 쓴다.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_pd = pd.read_csv('C:\\\\Users\\\\JINHYO\\\\.keras\\\\datasets\\\\auto-mpg.data', header=None,\n",
    "                     sep=' ', engine='python', comment='\\t', skipinitialspace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86.00</td>\n",
       "      <td>2790.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52.00</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.00</td>\n",
       "      <td>2295.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82.00</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0  1      2      3       4     5   6  7\n",
       "0    18.0  8  307.0  130.0  3504.0  12.0  70  1\n",
       "1    15.0  8  350.0  165.0  3693.0  11.5  70  1\n",
       "2    18.0  8  318.0  150.0  3436.0  11.0  70  1\n",
       "3    16.0  8  304.0  150.0  3433.0  12.0  70  1\n",
       "4    17.0  8  302.0  140.0  3449.0  10.5  70  1\n",
       "..    ... ..    ...    ...     ...   ...  .. ..\n",
       "393  27.0  4  140.0  86.00  2790.0  15.6  82  1\n",
       "394  44.0  4   97.0  52.00  2130.0  24.6  82  2\n",
       "395  32.0  4  135.0  84.00  2295.0  11.6  82  1\n",
       "396  28.0  4  120.0  79.00  2625.0  18.6  82  1\n",
       "397  31.0  4  119.0  82.00  2720.0  19.4  82  1\n",
       "\n",
       "[398 rows x 8 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_raw = mpg_pd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398 entries, 0 to 397\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       398 non-null    float64\n",
      " 1   1       398 non-null    int64  \n",
      " 2   2       398 non-null    float64\n",
      " 3   3       398 non-null    object \n",
      " 4   4       398 non-null    float64\n",
      " 5   5       398 non-null    float64\n",
      " 6   6       398 non-null    int64  \n",
      " 7   7       398 non-null    int64  \n",
      "dtypes: float64(4), int64(3), object(1)\n",
      "memory usage: 25.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# column 3은 object 타입이다.\n",
    "mpg_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### na 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>25.0</td>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>?</td>\n",
       "      <td>2046.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>200.0</td>\n",
       "      <td>?</td>\n",
       "      <td>2875.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>40.9</td>\n",
       "      <td>4</td>\n",
       "      <td>85.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1835.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>23.6</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>?</td>\n",
       "      <td>2905.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>34.5</td>\n",
       "      <td>4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>?</td>\n",
       "      <td>2320.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>23.0</td>\n",
       "      <td>4</td>\n",
       "      <td>151.0</td>\n",
       "      <td>?</td>\n",
       "      <td>3035.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0  1      2  3       4     5   6  7\n",
       "32   25.0  4   98.0  ?  2046.0  19.0  71  1\n",
       "126  21.0  6  200.0  ?  2875.0  17.0  74  1\n",
       "330  40.9  4   85.0  ?  1835.0  17.3  80  2\n",
       "336  23.6  4  140.0  ?  2905.0  14.3  80  1\n",
       "354  34.5  4  100.0  ?  2320.0  15.8  81  2\n",
       "374  23.0  4  151.0  ?  3035.0  20.5  82  1"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이번 데이터에는 missing value가 ?로 되어 있다.\n",
    "mpg[mpg[3]=='?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# na_values 옵션은 데이터를 불러들일 때 특정 문자를 NaN으로 처리한다.\n",
    "mpg_pd = pd.read_csv('C:\\\\Users\\\\JINHYO\\\\.keras\\\\datasets\\\\auto-mpg.data', header=None,\n",
    "                 sep=' ', engine='python', comment='\\t', na_values='?', skipinitialspace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as mino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x29b422824a8>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mino.matrix(mpg_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1      False\n",
       "2      False\n",
       "3      False\n",
       "4      False\n",
       "       ...  \n",
       "393    False\n",
       "394    False\n",
       "395    False\n",
       "396    False\n",
       "397    False\n",
       "Name: 3, Length: 398, dtype: bool"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg_pd[3].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg_pd[3].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4      5      6      7\n",
       "0    False  False  False  False  False  False  False  False\n",
       "1    False  False  False  False  False  False  False  False\n",
       "2    False  False  False  False  False  False  False  False\n",
       "3    False  False  False  False  False  False  False  False\n",
       "4    False  False  False  False  False  False  False  False\n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...\n",
       "393  False  False  False  False  False  False  False  False\n",
       "394  False  False  False  False  False  False  False  False\n",
       "395  False  False  False  False  False  False  False  False\n",
       "396  False  False  False  False  False  False  False  False\n",
       "397  False  False  False  False  False  False  False  False\n",
       "\n",
       "[398 rows x 8 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isna와 같다.\n",
    "mpg_pd.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값은 삭제하기로 한다. \n",
    "mpg_pd.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    245\n",
       "3     79\n",
       "2     68\n",
       "Name: 7, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value_counts는 값 종류별로 개수를 세어준다.\n",
    "mpg_pd[7].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 2, 2, 2,\n",
       "       2, 2, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 2, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 2, 2, 2, 1, 3, 3, 1, 3, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 3,\n",
       "       3, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 3, 1, 1, 1, 1, 3, 1, 3, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 1, 2, 2, 3, 3, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 2, 3, 1, 2, 1, 2,\n",
       "       2, 2, 2, 3, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1,\n",
       "       1, 1, 1, 2, 3, 3, 1, 2, 1, 2, 3, 2, 1, 1, 1, 1, 3, 1, 2, 1, 3, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 3, 2, 3, 2, 3,\n",
       "       2, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 3, 3, 1, 3, 1, 1, 3, 2, 2, 2, 2, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 2, 3, 1, 1, 2, 1, 2, 1, 1, 1, 3, 2, 1, 1, 1, 1, 2,\n",
       "       3, 1, 3, 1, 1, 1, 1, 2, 3, 3, 3, 3, 3, 1, 3, 2, 2, 2, 2, 3, 3, 2,\n",
       "       3, 3, 2, 3, 1, 1, 1, 1, 1, 3, 1, 3, 3, 3, 3, 3, 1, 1, 1, 2, 3, 3,\n",
       "       3, 3, 2, 2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 1, 1,\n",
       "       3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# values는 numpy로 바꿔준다.\n",
    "mpg_pd[7].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensorflow에서 OneHotEncoding을 만들어주는 함수\n",
    "# 0을 기점으로 One Hot Encoding한다. 그래서 값 종류는 3개이지만 열이 4개이다.\n",
    "tf.keras.utils.to_categorical(mpg_pd[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 392 entries, 0 to 397\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       392 non-null    float64\n",
      " 1   1       392 non-null    int64  \n",
      " 2   2       392 non-null    float64\n",
      " 3   3       392 non-null    float64\n",
      " 4   4       392 non-null    float64\n",
      " 5   5       392 non-null    float64\n",
      " 6   6       392 non-null    int64  \n",
      " 7   7       392 non-null    int64  \n",
      "dtypes: float64(5), int64(3)\n",
      "memory usage: 27.6 KB\n"
     ]
    }
   ],
   "source": [
    "# 이번에는 원핫인코딩을 안한 상태로 학습해본다.\n",
    "# 안 해도 학습은 된다. \n",
    "# 라벨인코딩은 순서에 따라 값의 크기 차이 때문에 문제가 발생할 수 있고\n",
    "# 원핫인코딩은 차원의 저주가 발생할 수 있다. \n",
    "mpg_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 One Hot Encoding을 안하고 학습해본 후에 One Hot Encoding의 효과를 비교해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalization 여부"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 normalization을 안하고 학습해본 후에 normalization의 효과를 비교해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Add in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(7,)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러가지로 테스트할 때, 모델 만드는 함수를 이용하면 편하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(7,)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(1)\n",
    "            ]\n",
    "    )\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 64)                512       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문자열 있을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw 데이터를 학습하면 어떻게 되는지 확인해보자.\n",
    "# Error\n",
    "model.fit(mpg_raw.iloc[:, 1:], mpg_raw.iloc[:, 0], epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398 entries, 0 to 397\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       398 non-null    float64\n",
      " 1   1       398 non-null    int64  \n",
      " 2   2       398 non-null    float64\n",
      " 3   3       398 non-null    object \n",
      " 4   4       398 non-null    float64\n",
      " 5   5       398 non-null    float64\n",
      " 6   6       398 non-null    int64  \n",
      " 7   7       398 non-null    int64  \n",
      "dtypes: float64(4), int64(3), object(1)\n",
      "memory usage: 25.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# 문자열이 있으면 학습이 안된다. (fit할 때 에러가 발생함)\n",
    "# dtype object가 있으면 안된다.\n",
    "mpg_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type casting (형 변환)\n",
    "# 숫자형으로 바꿔준다.\n",
    "# coerce 옵션은 숫자로 바꿀 수 없는 값이 있으면 NaN으로 처리한다.\n",
    "mpg_raw[3] = pd.to_numeric(mpg_raw[3], 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398 entries, 0 to 397\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       398 non-null    float64\n",
      " 1   1       398 non-null    int64  \n",
      " 2   2       398 non-null    float64\n",
      " 3   3       392 non-null    float64\n",
      " 4   4       398 non-null    float64\n",
      " 5   5       398 non-null    float64\n",
      " 6   6       398 non-null    int64  \n",
      " 7   7       398 non-null    int64  \n",
      "dtypes: float64(5), int64(3)\n",
      "memory usage: 25.0 KB\n"
     ]
    }
   ],
   "source": [
    "mpg_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### na 있을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0604 21:08:34.458441 27000 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 398 samples\n",
      "Epoch 1/5\n",
      "398/398 [==============================] - 0s 55us/sample - loss: nan - mse: nan - mae: nan    \n",
      "Epoch 2/5\n",
      "398/398 [==============================] - 0s 58us/sample - loss: nan - mse: nan - mae: nan\n",
      "Epoch 3/5\n",
      "398/398 [==============================] - 0s 53us/sample - loss: nan - mse: nan - mae: nan\n",
      "Epoch 4/5\n",
      "398/398 [==============================] - 0s 58us/sample - loss: nan - mse: nan - mae: nan\n",
      "Epoch 5/5\n",
      "398/398 [==============================] - 0s 53us/sample - loss: nan - mse: nan - mae: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29b4124ada0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn은 NAN값이 있으면 fit할 때 에러가 발생했다.\n",
    "# tensorflow는 NAN값이 있으면 fit할 때 에러는 발생하지 않지만, 학습이 안된다.(loss가 nan)\n",
    "# CPU일떄는 안되는데 GPU일때는 되는 경우도 있다.\n",
    "# 주의해야한다.\n",
    "model.fit(mpg_raw.iloc[:, 1:], mpg_raw.iloc[:, 0], epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 정제 후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0604 21:40:45.709057 27000 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 392 samples\n",
      "Epoch 1/100\n",
      "Executing op __inference_keras_scratch_graph_105335 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "392/392 [==============================] - 5s 13ms/sample - loss: 24318.5837 - mse: 24318.5840 - mae: 125.9208\n",
      "Epoch 2/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 3453.8975 - mse: 3453.8977 - mae: 49.7140\n",
      "Epoch 3/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 1167.0484 - mse: 1167.0483 - mae: 30.8153\n",
      "Epoch 4/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 430.0159 - mse: 430.0159 - mae: 16.6676\n",
      "Epoch 5/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 237.0932 - mse: 237.0932 - mae: 12.9695\n",
      "Epoch 6/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 189.6533 - mse: 189.6533 - mae: 11.3901\n",
      "Epoch 7/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 159.7198 - mse: 159.7198 - mae: 10.3619\n",
      "Epoch 8/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 151.1108 - mse: 151.1108 - mae: 10.1502\n",
      "Epoch 9/100\n",
      "392/392 [==============================] - 0s 56us/sample - loss: 150.1461 - mse: 150.1461 - mae: 10.2525\n",
      "Epoch 10/100\n",
      "392/392 [==============================] - 0s 56us/sample - loss: 145.0868 - mse: 145.0868 - mae: 10.0199\n",
      "Epoch 11/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 138.0687 - mse: 138.0687 - mae: 9.7061\n",
      "Epoch 12/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 133.6891 - mse: 133.6891 - mae: 9.4955\n",
      "Epoch 13/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 127.3724 - mse: 127.3724 - mae: 9.2574\n",
      "Epoch 14/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 128.2128 - mse: 128.2128 - mae: 9.1821\n",
      "Epoch 15/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 122.4826 - mse: 122.4826 - mae: 9.0560\n",
      "Epoch 16/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 120.0473 - mse: 120.0473 - mae: 8.9582\n",
      "Epoch 17/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 113.1951 - mse: 113.1951 - mae: 8.6255\n",
      "Epoch 18/100\n",
      "392/392 [==============================] - 0s 87us/sample - loss: 106.9872 - mse: 106.9872 - mae: 8.3746\n",
      "Epoch 19/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 104.1933 - mse: 104.1933 - mae: 8.2185\n",
      "Epoch 20/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 100.2253 - mse: 100.2253 - mae: 8.1256\n",
      "Epoch 21/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 96.2489 - mse: 96.2489 - mae: 7.9307\n",
      "Epoch 22/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 104.8804 - mse: 104.8804 - mae: 8.2844\n",
      "Epoch 23/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 94.9015 - mse: 94.9015 - mae: 7.7757\n",
      "Epoch 24/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 89.4568 - mse: 89.4568 - mae: 7.6071\n",
      "Epoch 25/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 84.8807 - mse: 84.8807 - mae: 7.2857\n",
      "Epoch 26/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 88.7116 - mse: 88.7116 - mae: 7.5704\n",
      "Epoch 27/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 82.1968 - mse: 82.1968 - mae: 7.1361\n",
      "Epoch 28/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 74.1977 - mse: 74.1977 - mae: 6.7749\n",
      "Epoch 29/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 72.8883 - mse: 72.8883 - mae: 6.7538\n",
      "Epoch 30/100\n",
      "392/392 [==============================] - 0s 92us/sample - loss: 72.6725 - mse: 72.6725 - mae: 6.7182\n",
      "Epoch 31/100\n",
      "392/392 [==============================] - 0s 84us/sample - loss: 71.1101 - mse: 71.1101 - mae: 6.6432\n",
      "Epoch 32/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 70.4989 - mse: 70.4989 - mae: 6.6408\n",
      "Epoch 33/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 67.1291 - mse: 67.1291 - mae: 6.4287\n",
      "Epoch 34/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 63.3186 - mse: 63.3186 - mae: 6.2407\n",
      "Epoch 35/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 64.5390 - mse: 64.5390 - mae: 6.3366\n",
      "Epoch 36/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 62.9999 - mse: 62.9999 - mae: 6.1804\n",
      "Epoch 37/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 57.8684 - mse: 57.8684 - mae: 5.8808\n",
      "Epoch 38/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 67.1734 - mse: 67.1734 - mae: 6.3624\n",
      "Epoch 39/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 64.7943 - mse: 64.7943 - mae: 6.3672\n",
      "Epoch 40/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 57.6426 - mse: 57.6426 - mae: 5.8905\n",
      "Epoch 41/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 55.6134 - mse: 55.6134 - mae: 5.7767\n",
      "Epoch 42/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 55.0626 - mse: 55.0626 - mae: 5.7378\n",
      "Epoch 43/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 53.1636 - mse: 53.1636 - mae: 5.6758\n",
      "Epoch 44/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 51.8638 - mse: 51.8638 - mae: 5.5466\n",
      "Epoch 45/100\n",
      "392/392 [==============================] - 0s 54us/sample - loss: 50.9432 - mse: 50.9432 - mae: 5.5219\n",
      "Epoch 46/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 52.0066 - mse: 52.0066 - mae: 5.6706\n",
      "Epoch 47/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 54.1823 - mse: 54.1823 - mae: 5.6652\n",
      "Epoch 48/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 50.9855 - mse: 50.9855 - mae: 5.5360\n",
      "Epoch 49/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 50.3432 - mse: 50.3432 - mae: 5.5294\n",
      "Epoch 50/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 52.7846 - mse: 52.7846 - mae: 5.6347\n",
      "Epoch 51/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 57.5424 - mse: 57.5424 - mae: 5.9138\n",
      "Epoch 52/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 46.8973 - mse: 46.8973 - mae: 5.3101\n",
      "Epoch 53/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 46.6283 - mse: 46.6283 - mae: 5.2770\n",
      "Epoch 54/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 46.7221 - mse: 46.7221 - mae: 5.3341\n",
      "Epoch 55/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 46.0731 - mse: 46.0731 - mae: 5.2494\n",
      "Epoch 56/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 44.9031 - mse: 44.9032 - mae: 5.1595\n",
      "Epoch 57/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 45.4381 - mse: 45.4381 - mae: 5.1930\n",
      "Epoch 58/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 45.3595 - mse: 45.3595 - mae: 5.1685\n",
      "Epoch 59/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 42.5290 - mse: 42.5290 - mae: 4.9774\n",
      "Epoch 60/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 42.8928 - mse: 42.8928 - mae: 5.0424\n",
      "Epoch 61/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 41.5119 - mse: 41.5119 - mae: 4.9508\n",
      "Epoch 62/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 41.0900 - mse: 41.0900 - mae: 4.9213\n",
      "Epoch 63/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 42.7833 - mse: 42.7833 - mae: 4.9976\n",
      "Epoch 64/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 41.0308 - mse: 41.0308 - mae: 4.9298\n",
      "Epoch 65/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 40.5999 - mse: 40.5999 - mae: 4.8842\n",
      "Epoch 66/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 39.4940 - mse: 39.4940 - mae: 4.7993\n",
      "Epoch 67/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 39.5200 - mse: 39.5201 - mae: 4.7709\n",
      "Epoch 68/100\n",
      "392/392 [==============================] - 0s 56us/sample - loss: 39.5061 - mse: 39.5061 - mae: 4.8631\n",
      "Epoch 69/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 41.8125 - mse: 41.8125 - mae: 4.9889\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392/392 [==============================] - 0s 59us/sample - loss: 54.9287 - mse: 54.9287 - mae: 5.8411\n",
      "Epoch 71/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 39.2287 - mse: 39.2287 - mae: 4.8255\n",
      "Epoch 72/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 39.5436 - mse: 39.5436 - mae: 4.8343\n",
      "Epoch 73/100\n",
      "392/392 [==============================] - 0s 87us/sample - loss: 48.0572 - mse: 48.0572 - mae: 5.5064\n",
      "Epoch 74/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 53.3969 - mse: 53.3969 - mae: 5.8928\n",
      "Epoch 75/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 58.3541 - mse: 58.3541 - mae: 6.0429\n",
      "Epoch 76/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 44.6065 - mse: 44.6065 - mae: 5.3274\n",
      "Epoch 77/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 40.9633 - mse: 40.9633 - mae: 4.9849\n",
      "Epoch 78/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 38.7034 - mse: 38.7034 - mae: 4.8026\n",
      "Epoch 79/100\n",
      "392/392 [==============================] - 0s 87us/sample - loss: 39.2224 - mse: 39.2224 - mae: 4.8381\n",
      "Epoch 80/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 53.3918 - mse: 53.3918 - mae: 5.7788\n",
      "Epoch 81/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 45.9785 - mse: 45.9785 - mae: 5.4467\n",
      "Epoch 82/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 38.3787 - mse: 38.3787 - mae: 4.9237\n",
      "Epoch 83/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 34.7325 - mse: 34.7325 - mae: 4.4442\n",
      "Epoch 84/100\n",
      "392/392 [==============================] - 0s 87us/sample - loss: 32.3607 - mse: 32.3607 - mae: 4.2961\n",
      "Epoch 85/100\n",
      "392/392 [==============================] - 0s 84us/sample - loss: 33.0693 - mse: 33.0693 - mae: 4.4317\n",
      "Epoch 86/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 33.8957 - mse: 33.8957 - mae: 4.4260\n",
      "Epoch 87/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 33.7234 - mse: 33.7234 - mae: 4.4103\n",
      "Epoch 88/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 32.9388 - mse: 32.9388 - mae: 4.3728\n",
      "Epoch 89/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 32.9878 - mse: 32.9878 - mae: 4.3618\n",
      "Epoch 90/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 31.9109 - mse: 31.9109 - mae: 4.3711\n",
      "Epoch 91/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 32.9700 - mse: 32.9700 - mae: 4.4230\n",
      "Epoch 92/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 32.3975 - mse: 32.3975 - mae: 4.3357\n",
      "Epoch 93/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 29.1936 - mse: 29.1936 - mae: 4.0791\n",
      "Epoch 94/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 29.1194 - mse: 29.1194 - mae: 4.0836\n",
      "Epoch 95/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 30.8939 - mse: 30.8939 - mae: 4.2537\n",
      "Epoch 96/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 29.6159 - mse: 29.6159 - mae: 4.1794\n",
      "Epoch 97/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 27.4455 - mse: 27.4455 - mae: 3.9421\n",
      "Epoch 98/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 28.3713 - mse: 28.3713 - mae: 4.0297\n",
      "Epoch 99/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 27.8616 - mse: 27.8615 - mae: 3.9786\n",
      "Epoch 100/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 26.7660 - mse: 26.7660 - mae: 3.9046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29b46c6feb8>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문자열 데이터 없고 missing value 없으면 학습 잘 된다.\n",
    "# OnehotEncoding하지 않았으므로 LabelEncoding과 같다.\n",
    "model_pd = create_model()\n",
    "model_pd.fit(mpg_pd.iloc[:, 1:], mpg_pd.iloc[:, 0], epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능 : loss: 26.7660 - mse: 26.7660 - mae: 3.9046"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding 했을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoding을 하고 학습 후 비교해보자.\n",
    "mpg_oh = pd.concat([mpg_pd, pd.get_dummies(mpg_pd[7])], axis=1).drop(columns=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoding 후에는 feature 개수가 달라지므로 input_shape이 달라진다.\n",
    "def create_model2():\n",
    "    model = tf.keras.models.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(9,)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(1)\n",
    "            ]\n",
    "    )\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0604 21:41:01.696512 27000 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 392 samples\n",
      "Epoch 1/100\n",
      "Executing op __inference_keras_scratch_graph_114164 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "392/392 [==============================] - 6s 15ms/sample - loss: 1475.5866 - mse: 1475.5868 - mae: 30.9225\n",
      "Epoch 2/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 525.8001 - mse: 525.8000 - mae: 18.6092\n",
      "Epoch 3/100\n",
      "392/392 [==============================] - 0s 56us/sample - loss: 377.7707 - mse: 377.7707 - mae: 16.3299\n",
      "Epoch 4/100\n",
      "392/392 [==============================] - 0s 56us/sample - loss: 288.8817 - mse: 288.8817 - mae: 14.6355\n",
      "Epoch 5/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 212.7461 - mse: 212.7461 - mae: 12.2652\n",
      "Epoch 6/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 171.3279 - mse: 171.3279 - mae: 10.9357\n",
      "Epoch 7/100\n",
      "392/392 [==============================] - 0s 56us/sample - loss: 174.9789 - mse: 174.9789 - mae: 10.6594\n",
      "Epoch 8/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 108.4366 - mse: 108.4366 - mae: 8.5017\n",
      "Epoch 9/100\n",
      "392/392 [==============================] - 0s 56us/sample - loss: 74.4498 - mse: 74.4498 - mae: 7.0135\n",
      "Epoch 10/100\n",
      "392/392 [==============================] - 0s 56us/sample - loss: 58.9253 - mse: 58.9253 - mae: 6.1557\n",
      "Epoch 11/100\n",
      "392/392 [==============================] - 0s 54us/sample - loss: 45.4679 - mse: 45.4679 - mae: 5.3071\n",
      "Epoch 12/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 36.0941 - mse: 36.0941 - mae: 4.5748\n",
      "Epoch 13/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 36.1354 - mse: 36.1354 - mae: 4.6712\n",
      "Epoch 14/100\n",
      "392/392 [==============================] - 0s 56us/sample - loss: 31.2977 - mse: 31.2977 - mae: 4.3488\n",
      "Epoch 15/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 24.8603 - mse: 24.8603 - mae: 3.7918\n",
      "Epoch 16/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 22.2479 - mse: 22.2479 - mae: 3.4819\n",
      "Epoch 17/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 19.4551 - mse: 19.4551 - mae: 3.1783\n",
      "Epoch 18/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 21.0385 - mse: 21.0385 - mae: 3.4602\n",
      "Epoch 19/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 20.0015 - mse: 20.0015 - mae: 3.3241\n",
      "Epoch 20/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 20.9660 - mse: 20.9660 - mae: 3.5027\n",
      "Epoch 21/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 20.1360 - mse: 20.1360 - mae: 3.3871\n",
      "Epoch 22/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 18.4605 - mse: 18.4605 - mae: 3.2294\n",
      "Epoch 23/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 17.7714 - mse: 17.7714 - mae: 3.1268\n",
      "Epoch 24/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 17.3667 - mse: 17.3667 - mae: 3.0454\n",
      "Epoch 25/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 19.8097 - mse: 19.8097 - mae: 3.3771\n",
      "Epoch 26/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 21.2309 - mse: 21.2309 - mae: 3.5837\n",
      "Epoch 27/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 19.0550 - mse: 19.0550 - mae: 3.2861\n",
      "Epoch 28/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 32.4639 - mse: 32.4639 - mae: 4.4885\n",
      "Epoch 29/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 29.7289 - mse: 29.7289 - mae: 4.2859\n",
      "Epoch 30/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 18.5947 - mse: 18.5947 - mae: 3.3289\n",
      "Epoch 31/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 17.9746 - mse: 17.9746 - mae: 3.1964\n",
      "Epoch 32/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 16.6816 - mse: 16.6816 - mae: 3.1019\n",
      "Epoch 33/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 16.5776 - mse: 16.5776 - mae: 3.0534\n",
      "Epoch 34/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 16.8335 - mse: 16.8335 - mae: 3.0937\n",
      "Epoch 35/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 17.0462 - mse: 17.0462 - mae: 3.1263\n",
      "Epoch 36/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 16.9108 - mse: 16.9108 - mae: 3.1206\n",
      "Epoch 37/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 18.8724 - mse: 18.8724 - mae: 3.2881\n",
      "Epoch 38/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 18.3148 - mse: 18.3148 - mae: 3.2187\n",
      "Epoch 39/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 17.4646 - mse: 17.4646 - mae: 3.2259\n",
      "Epoch 40/100\n",
      "392/392 [==============================] - 0s 89us/sample - loss: 17.1297 - mse: 17.1297 - mae: 3.1461\n",
      "Epoch 41/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 15.8941 - mse: 15.8941 - mae: 3.0618\n",
      "Epoch 42/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 15.2307 - mse: 15.2307 - mae: 2.9745\n",
      "Epoch 43/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 25.4119 - mse: 25.4119 - mae: 4.0033\n",
      "Epoch 44/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 30.3584 - mse: 30.3584 - mae: 4.3890\n",
      "Epoch 45/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 17.6250 - mse: 17.6250 - mae: 3.2700\n",
      "Epoch 46/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 18.7630 - mse: 18.7630 - mae: 3.3566\n",
      "Epoch 47/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 23.0087 - mse: 23.0087 - mae: 3.8592\n",
      "Epoch 48/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 24.1912 - mse: 24.1912 - mae: 3.9197\n",
      "Epoch 49/100\n",
      "392/392 [==============================] - 0s 84us/sample - loss: 30.0508 - mse: 30.0508 - mae: 4.4124\n",
      "Epoch 50/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 21.5369 - mse: 21.5369 - mae: 3.6038\n",
      "Epoch 51/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 14.4281 - mse: 14.4281 - mae: 2.8814\n",
      "Epoch 52/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 19.2235 - mse: 19.2235 - mae: 3.4736\n",
      "Epoch 53/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 31.2722 - mse: 31.2722 - mae: 4.6445\n",
      "Epoch 54/100\n",
      "392/392 [==============================] - 0s 89us/sample - loss: 18.2690 - mse: 18.2690 - mae: 3.2994\n",
      "Epoch 55/100\n",
      "392/392 [==============================] - 0s 87us/sample - loss: 18.4381 - mse: 18.4381 - mae: 3.3471\n",
      "Epoch 56/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 15.5513 - mse: 15.5513 - mae: 3.0659\n",
      "Epoch 57/100\n",
      "392/392 [==============================] - 0s 87us/sample - loss: 14.6148 - mse: 14.6148 - mae: 2.9235\n",
      "Epoch 58/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 20.0030 - mse: 20.0030 - mae: 3.5554\n",
      "Epoch 59/100\n",
      "392/392 [==============================] - 0s 84us/sample - loss: 15.3079 - mse: 15.3079 - mae: 3.0610\n",
      "Epoch 60/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 13.8972 - mse: 13.8972 - mae: 2.8707\n",
      "Epoch 61/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 21.8611 - mse: 21.8611 - mae: 3.6335\n",
      "Epoch 62/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 37.3896 - mse: 37.3896 - mae: 4.9403\n",
      "Epoch 63/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 64.7714 - mse: 64.7714 - mae: 6.7881\n",
      "Epoch 64/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 42.9845 - mse: 42.9845 - mae: 5.4002\n",
      "Epoch 65/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 31.6601 - mse: 31.6601 - mae: 4.6216\n",
      "Epoch 66/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 21.0621 - mse: 21.0621 - mae: 3.7204\n",
      "Epoch 67/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 51.9963 - mse: 51.9963 - mae: 6.0512\n",
      "Epoch 68/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 25.8103 - mse: 25.8103 - mae: 4.0676\n",
      "Epoch 69/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 16.7983 - mse: 16.7983 - mae: 3.2455\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392/392 [==============================] - 0s 66us/sample - loss: 30.4692 - mse: 30.4692 - mae: 4.5619\n",
      "Epoch 71/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 16.8011 - mse: 16.8011 - mae: 3.1912\n",
      "Epoch 72/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 17.7568 - mse: 17.7568 - mae: 3.3425\n",
      "Epoch 73/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 45.3285 - mse: 45.3285 - mae: 5.6363\n",
      "Epoch 74/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 15.3939 - mse: 15.3939 - mae: 3.1006\n",
      "Epoch 75/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 21.8090 - mse: 21.8090 - mae: 3.6493\n",
      "Epoch 76/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 22.7277 - mse: 22.7277 - mae: 3.8543\n",
      "Epoch 77/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 22.3650 - mse: 22.3650 - mae: 3.8053\n",
      "Epoch 78/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 21.3174 - mse: 21.3174 - mae: 3.7186\n",
      "Epoch 79/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 22.6790 - mse: 22.6790 - mae: 3.7187\n",
      "Epoch 80/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 14.4653 - mse: 14.4653 - mae: 2.9000\n",
      "Epoch 81/100\n",
      "392/392 [==============================] - ETA: 0s - loss: 9.1608 - mse: 9.1608 - mae: 2.383 - 0s 77us/sample - loss: 13.4570 - mse: 13.4570 - mae: 2.8172\n",
      "Epoch 82/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 13.5539 - mse: 13.5539 - mae: 2.8584\n",
      "Epoch 83/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 17.1138 - mse: 17.1138 - mae: 3.2692\n",
      "Epoch 84/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 16.0024 - mse: 16.0024 - mae: 3.0935\n",
      "Epoch 85/100\n",
      "392/392 [==============================] - 0s 84us/sample - loss: 13.2840 - mse: 13.2840 - mae: 2.7769\n",
      "Epoch 86/100\n",
      "392/392 [==============================] - 0s 92us/sample - loss: 20.1302 - mse: 20.1302 - mae: 3.4990\n",
      "Epoch 87/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 21.8038 - mse: 21.8038 - mae: 3.6757\n",
      "Epoch 88/100\n",
      "392/392 [==============================] - 0s 84us/sample - loss: 25.6787 - mse: 25.6787 - mae: 4.1432\n",
      "Epoch 89/100\n",
      "392/392 [==============================] - 0s 84us/sample - loss: 16.9301 - mse: 16.9301 - mae: 3.2401\n",
      "Epoch 90/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 18.2711 - mse: 18.2711 - mae: 3.3892\n",
      "Epoch 91/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 17.0651 - mse: 17.0651 - mae: 3.2022\n",
      "Epoch 92/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 23.7240 - mse: 23.7240 - mae: 3.9094\n",
      "Epoch 93/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 43.3814 - mse: 43.3814 - mae: 5.3649\n",
      "Epoch 94/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 23.3637 - mse: 23.3637 - mae: 3.8386\n",
      "Epoch 95/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 23.1271 - mse: 23.1271 - mae: 3.7980\n",
      "Epoch 96/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 14.8927 - mse: 14.8927 - mae: 2.9236\n",
      "Epoch 97/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 19.4078 - mse: 19.4078 - mae: 3.5373\n",
      "Epoch 98/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 17.6015 - mse: 17.6015 - mae: 3.3125\n",
      "Epoch 99/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 16.1861 - mse: 16.1861 - mae: 3.0836\n",
      "Epoch 100/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 18.1604 - mse: 18.1604 - mae: 3.3245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29b46c31e10>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_oh = create_model2()\n",
    "model_oh.fit(mpg_oh.iloc[:, 1:], mpg_oh.iloc[:, 0], epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능 : loss: 18.1604 - mse: 18.1604 - mae: 3.3245"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalization 후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mm.fit_transform(mpg_pd.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Train on 392 samples\n",
      "Epoch 1/100\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_initialize_variables_129889 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_130112 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "392/392 [==============================] - 5s 13ms/sample - loss: 598.9153 - mse: 598.9153 - mae: 23.1674\n",
      "Epoch 2/100\n",
      "392/392 [==============================] - 0s 87us/sample - loss: 569.2029 - mse: 569.2029 - mae: 22.4813\n",
      "Epoch 3/100\n",
      "392/392 [==============================] - 0s 102us/sample - loss: 527.2217 - mse: 527.2217 - mae: 21.4639\n",
      "Epoch 4/100\n",
      "392/392 [==============================] - 0s 87us/sample - loss: 465.9539 - mse: 465.9539 - mae: 19.8696\n",
      "Epoch 5/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 381.0828 - mse: 381.0828 - mae: 17.3819\n",
      "Epoch 6/100\n",
      "392/392 [==============================] - 0s 94us/sample - loss: 277.5448 - mse: 277.5448 - mae: 13.9741\n",
      "Epoch 7/100\n",
      "392/392 [==============================] - 0s 89us/sample - loss: 184.7576 - mse: 184.7576 - mae: 11.1929\n",
      "Epoch 8/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 126.4431 - mse: 126.4431 - mae: 9.5855\n",
      "Epoch 9/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 102.8686 - mse: 102.8686 - mae: 8.7959\n",
      "Epoch 10/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 84.0254 - mse: 84.0254 - mae: 7.9111\n",
      "Epoch 11/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 67.9571 - mse: 67.9571 - mae: 7.0671\n",
      "Epoch 12/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 54.7480 - mse: 54.7480 - mae: 6.2458\n",
      "Epoch 13/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 44.6004 - mse: 44.6004 - mae: 5.5560\n",
      "Epoch 14/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 36.9143 - mse: 36.9143 - mae: 4.9963\n",
      "Epoch 15/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 31.7575 - mse: 31.7575 - mae: 4.5843\n",
      "Epoch 16/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 28.3244 - mse: 28.3244 - mae: 4.2851\n",
      "Epoch 17/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 25.8478 - mse: 25.8478 - mae: 4.0297\n",
      "Epoch 18/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 24.3241 - mse: 24.3241 - mae: 3.8663\n",
      "Epoch 19/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 23.4303 - mse: 23.4303 - mae: 3.7910\n",
      "Epoch 20/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 22.3252 - mse: 22.3252 - mae: 3.6768\n",
      "Epoch 21/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 21.6990 - mse: 21.6990 - mae: 3.5951\n",
      "Epoch 22/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 21.1547 - mse: 21.1547 - mae: 3.5599\n",
      "Epoch 23/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 20.5955 - mse: 20.5955 - mae: 3.5192\n",
      "Epoch 24/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 20.1020 - mse: 20.1020 - mae: 3.4661\n",
      "Epoch 25/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 19.7318 - mse: 19.7318 - mae: 3.4475\n",
      "Epoch 26/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 19.2021 - mse: 19.2021 - mae: 3.4037\n",
      "Epoch 27/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 18.8018 - mse: 18.8018 - mae: 3.3644\n",
      "Epoch 28/100\n",
      "392/392 [==============================] - 0s 84us/sample - loss: 18.4114 - mse: 18.4114 - mae: 3.3519\n",
      "Epoch 29/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 18.0316 - mse: 18.0316 - mae: 3.3258\n",
      "Epoch 30/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 17.8759 - mse: 17.8759 - mae: 3.2815\n",
      "Epoch 31/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 17.3120 - mse: 17.3120 - mae: 3.2457\n",
      "Epoch 32/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 16.9903 - mse: 16.9903 - mae: 3.2348\n",
      "Epoch 33/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 16.6995 - mse: 16.6995 - mae: 3.2067\n",
      "Epoch 34/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 16.3791 - mse: 16.3791 - mae: 3.1673\n",
      "Epoch 35/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 16.1371 - mse: 16.1371 - mae: 3.1357\n",
      "Epoch 36/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 15.7616 - mse: 15.7616 - mae: 3.1044\n",
      "Epoch 37/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 15.5772 - mse: 15.5772 - mae: 3.0992\n",
      "Epoch 38/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 15.3246 - mse: 15.3246 - mae: 3.0614\n",
      "Epoch 39/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 14.9816 - mse: 14.9816 - mae: 3.0399\n",
      "Epoch 40/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 14.7127 - mse: 14.7127 - mae: 3.0080\n",
      "Epoch 41/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 14.3881 - mse: 14.3881 - mae: 2.9727\n",
      "Epoch 42/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 14.2425 - mse: 14.2425 - mae: 2.9647\n",
      "Epoch 43/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 13.8731 - mse: 13.8731 - mae: 2.9135\n",
      "Epoch 44/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 13.6319 - mse: 13.6319 - mae: 2.8933\n",
      "Epoch 45/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 13.4437 - mse: 13.4437 - mae: 2.8616\n",
      "Epoch 46/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 13.0834 - mse: 13.0834 - mae: 2.8221\n",
      "Epoch 47/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 12.9241 - mse: 12.9241 - mae: 2.8081\n",
      "Epoch 48/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 12.8259 - mse: 12.8259 - mae: 2.7852\n",
      "Epoch 49/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 12.5021 - mse: 12.5021 - mae: 2.7475\n",
      "Epoch 50/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 12.2280 - mse: 12.2280 - mae: 2.7217\n",
      "Epoch 51/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 12.0202 - mse: 12.0202 - mae: 2.6896\n",
      "Epoch 52/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 11.9170 - mse: 11.9170 - mae: 2.6775\n",
      "Epoch 53/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 11.6001 - mse: 11.6001 - mae: 2.6399\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392/392 [==============================] - 0s 77us/sample - loss: 11.4466 - mse: 11.4466 - mae: 2.6156\n",
      "Epoch 55/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 11.3428 - mse: 11.3428 - mae: 2.6080\n",
      "Epoch 56/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 11.0949 - mse: 11.0949 - mae: 2.5663\n",
      "Epoch 57/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 10.9398 - mse: 10.9398 - mae: 2.5445\n",
      "Epoch 58/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 10.8446 - mse: 10.8446 - mae: 2.5305\n",
      "Epoch 59/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 10.6194 - mse: 10.6194 - mae: 2.5019\n",
      "Epoch 60/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 10.5456 - mse: 10.5456 - mae: 2.4812\n",
      "Epoch 61/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 10.2941 - mse: 10.2941 - mae: 2.4538\n",
      "Epoch 62/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 10.2367 - mse: 10.2367 - mae: 2.4501\n",
      "Epoch 63/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 10.0068 - mse: 10.0068 - mae: 2.4008\n",
      "Epoch 64/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 9.9040 - mse: 9.9040 - mae: 2.3810\n",
      "Epoch 65/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 10.0645 - mse: 10.0645 - mae: 2.4173\n",
      "Epoch 66/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 9.8103 - mse: 9.8103 - mae: 2.3514\n",
      "Epoch 67/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 9.6107 - mse: 9.6107 - mae: 2.3462\n",
      "Epoch 68/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 9.4165 - mse: 9.4165 - mae: 2.3159\n",
      "Epoch 69/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 9.3521 - mse: 9.3521 - mae: 2.2942\n",
      "Epoch 70/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 9.2473 - mse: 9.2473 - mae: 2.2877\n",
      "Epoch 71/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 9.1532 - mse: 9.1532 - mae: 2.2720\n",
      "Epoch 72/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 9.0961 - mse: 9.0961 - mae: 2.2695\n",
      "Epoch 73/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 9.1810 - mse: 9.1810 - mae: 2.2582\n",
      "Epoch 74/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 9.2154 - mse: 9.2154 - mae: 2.2810\n",
      "Epoch 75/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 8.9370 - mse: 8.9370 - mae: 2.2156\n",
      "Epoch 76/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 8.8639 - mse: 8.8639 - mae: 2.2257\n",
      "Epoch 77/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 8.7703 - mse: 8.7703 - mae: 2.2061\n",
      "Epoch 78/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 8.5817 - mse: 8.5817 - mae: 2.1854\n",
      "Epoch 79/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 8.5934 - mse: 8.5934 - mae: 2.1846\n",
      "Epoch 80/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 8.6878 - mse: 8.6878 - mae: 2.1782\n",
      "Epoch 81/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 8.4457 - mse: 8.4457 - mae: 2.1559\n",
      "Epoch 82/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 8.4866 - mse: 8.4866 - mae: 2.1733\n",
      "Epoch 83/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 8.4663 - mse: 8.4663 - mae: 2.1469\n",
      "Epoch 84/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 8.3828 - mse: 8.3828 - mae: 2.1593\n",
      "Epoch 85/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 8.4510 - mse: 8.4510 - mae: 2.1511\n",
      "Epoch 86/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 8.2733 - mse: 8.2733 - mae: 2.1346\n",
      "Epoch 87/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 8.2716 - mse: 8.2716 - mae: 2.1198\n",
      "Epoch 88/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 8.2585 - mse: 8.2585 - mae: 2.1392\n",
      "Epoch 89/100\n",
      "392/392 [==============================] - ETA: 0s - loss: 4.7442 - mse: 4.7442 - mae: 1.568 - 0s 71us/sample - loss: 8.1302 - mse: 8.1302 - mae: 2.0937\n",
      "Epoch 90/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 8.1485 - mse: 8.1485 - mae: 2.1153\n",
      "Epoch 91/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 8.0455 - mse: 8.0455 - mae: 2.0996\n",
      "Epoch 92/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 8.1519 - mse: 8.1519 - mae: 2.0981\n",
      "Epoch 93/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 8.2694 - mse: 8.2694 - mae: 2.1476\n",
      "Epoch 94/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 8.1017 - mse: 8.1017 - mae: 2.0848\n",
      "Epoch 95/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 7.9775 - mse: 7.9775 - mae: 2.0839\n",
      "Epoch 96/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 8.0247 - mse: 8.0247 - mae: 2.0773\n",
      "Epoch 97/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 7.9437 - mse: 7.9437 - mae: 2.0836\n",
      "Epoch 98/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 7.9457 - mse: 7.9457 - mae: 2.0629\n",
      "Epoch 99/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 7.8109 - mse: 7.8109 - mae: 2.0537\n",
      "Epoch 100/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 7.8081 - mse: 7.8081 - mae: 2.0508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29b48a295f8>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mm = create_model()\n",
    "model_mm.fit(X, mpg_pd[0].values, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능 : loss: 7.8081 - mse: 7.8081 - mae: 2.0508  \n",
    "\n",
    "model_pd, model_oh와 비교해보면, 첫번째 epoch부터 loss가 훨씬 줄어들어있다.  \n",
    "\n",
    "이 알고리즘은 태생적으로 값이 0 ~ 1 사이에 있을 때 가장 성능이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_oh = mm.fit_transform(mpg_oh.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_oh_mm = create_model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Train on 392 samples\n",
      "Epoch 1/100\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_initialize_variables_136309 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_136532 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "392/392 [==============================] - 6s 15ms/sample - loss: 587.4661 - mse: 587.4661 - mae: 22.9261\n",
      "Epoch 2/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 550.7235 - mse: 550.7234 - mae: 22.0788\n",
      "Epoch 3/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 498.5111 - mse: 498.5112 - mae: 20.7656\n",
      "Epoch 4/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 421.4042 - mse: 421.4043 - mae: 18.6465\n",
      "Epoch 5/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 320.1233 - mse: 320.1234 - mae: 15.4372\n",
      "Epoch 6/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 214.9825 - mse: 214.9825 - mae: 11.8470\n",
      "Epoch 7/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 146.5376 - mse: 146.5376 - mae: 10.2606\n",
      "Epoch 8/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 114.0858 - mse: 114.0858 - mae: 9.3526\n",
      "Epoch 9/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 91.9747 - mse: 91.9748 - mae: 8.3976\n",
      "Epoch 10/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 72.0781 - mse: 72.0781 - mae: 7.3644\n",
      "Epoch 11/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 56.0604 - mse: 56.0604 - mae: 6.4323\n",
      "Epoch 12/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 44.9204 - mse: 44.9204 - mae: 5.6639\n",
      "Epoch 13/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 36.0944 - mse: 36.0944 - mae: 5.0097\n",
      "Epoch 14/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 30.5262 - mse: 30.5262 - mae: 4.5487\n",
      "Epoch 15/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 26.3079 - mse: 26.3079 - mae: 4.1506\n",
      "Epoch 16/100\n",
      "392/392 [==============================] - 0s 84us/sample - loss: 23.3296 - mse: 23.3296 - mae: 3.8561\n",
      "Epoch 17/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 21.5197 - mse: 21.5197 - mae: 3.6765\n",
      "Epoch 18/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 19.7518 - mse: 19.7518 - mae: 3.4797\n",
      "Epoch 19/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 18.5452 - mse: 18.5452 - mae: 3.3474\n",
      "Epoch 20/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 17.6241 - mse: 17.6241 - mae: 3.2572\n",
      "Epoch 21/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 16.7896 - mse: 16.7896 - mae: 3.1761\n",
      "Epoch 22/100\n",
      "392/392 [==============================] - 0s 84us/sample - loss: 16.1695 - mse: 16.1695 - mae: 3.1228\n",
      "Epoch 23/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 15.6101 - mse: 15.6101 - mae: 3.0741\n",
      "Epoch 24/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 15.1375 - mse: 15.1375 - mae: 3.0299\n",
      "Epoch 25/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 14.6895 - mse: 14.6895 - mae: 2.9867\n",
      "Epoch 26/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 14.4081 - mse: 14.4081 - mae: 2.9561\n",
      "Epoch 27/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 13.9916 - mse: 13.9916 - mae: 2.9171\n",
      "Epoch 28/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 13.5847 - mse: 13.5847 - mae: 2.8717\n",
      "Epoch 29/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 13.4750 - mse: 13.4750 - mae: 2.8363\n",
      "Epoch 30/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 12.9803 - mse: 12.9803 - mae: 2.8019\n",
      "Epoch 31/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 12.7488 - mse: 12.7488 - mae: 2.7730\n",
      "Epoch 32/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 12.5243 - mse: 12.5243 - mae: 2.7437\n",
      "Epoch 33/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 12.3068 - mse: 12.3068 - mae: 2.7210\n",
      "Epoch 34/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 12.1038 - mse: 12.1038 - mae: 2.6868\n",
      "Epoch 35/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 11.8513 - mse: 11.8513 - mae: 2.6656\n",
      "Epoch 36/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 11.7059 - mse: 11.7059 - mae: 2.6446\n",
      "Epoch 37/100\n",
      "392/392 [==============================] - 0s 56us/sample - loss: 11.6140 - mse: 11.6140 - mae: 2.6378\n",
      "Epoch 38/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 11.3387 - mse: 11.3387 - mae: 2.5944\n",
      "Epoch 39/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 11.2571 - mse: 11.2571 - mae: 2.5709\n",
      "Epoch 40/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 10.9713 - mse: 10.9713 - mae: 2.5416\n",
      "Epoch 41/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 10.9380 - mse: 10.9380 - mae: 2.5484\n",
      "Epoch 42/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 10.8066 - mse: 10.8066 - mae: 2.5205\n",
      "Epoch 43/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 10.6299 - mse: 10.6299 - mae: 2.4960\n",
      "Epoch 44/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 10.4482 - mse: 10.4482 - mae: 2.4701\n",
      "Epoch 45/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 10.3091 - mse: 10.3091 - mae: 2.4473\n",
      "Epoch 46/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 10.1963 - mse: 10.1963 - mae: 2.4400\n",
      "Epoch 47/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 10.2010 - mse: 10.2010 - mae: 2.4275\n",
      "Epoch 48/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 9.9049 - mse: 9.9049 - mae: 2.3843\n",
      "Epoch 49/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 9.8379 - mse: 9.8379 - mae: 2.3663\n",
      "Epoch 50/100\n",
      "392/392 [==============================] - 0s 76us/sample - loss: 9.6949 - mse: 9.6949 - mae: 2.3525\n",
      "Epoch 51/100\n",
      "392/392 [==============================] - 0s 92us/sample - loss: 9.6221 - mse: 9.6221 - mae: 2.3430\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392/392 [==============================] - 0s 105us/sample - loss: 9.5691 - mse: 9.5691 - mae: 2.3381\n",
      "Epoch 53/100\n",
      "392/392 [==============================] - 0s 94us/sample - loss: 9.4894 - mse: 9.4894 - mae: 2.3065\n",
      "Epoch 54/100\n",
      "392/392 [==============================] - 0s 87us/sample - loss: 9.3328 - mse: 9.3328 - mae: 2.2993\n",
      "Epoch 55/100\n",
      "392/392 [==============================] - 0s 92us/sample - loss: 9.2591 - mse: 9.2591 - mae: 2.2869\n",
      "Epoch 56/100\n",
      "392/392 [==============================] - 0s 105us/sample - loss: 9.2168 - mse: 9.2168 - mae: 2.2803\n",
      "Epoch 57/100\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 9.1032 - mse: 9.1032 - mae: 2.2525\n",
      "Epoch 58/100\n",
      "392/392 [==============================] - 0s 87us/sample - loss: 9.0114 - mse: 9.0114 - mae: 2.2353\n",
      "Epoch 59/100\n",
      "392/392 [==============================] - 0s 84us/sample - loss: 8.9082 - mse: 8.9082 - mae: 2.2218\n",
      "Epoch 60/100\n",
      "392/392 [==============================] - 0s 130us/sample - loss: 8.8557 - mse: 8.8557 - mae: 2.2186\n",
      "Epoch 61/100\n",
      "392/392 [==============================] - 0s 153us/sample - loss: 8.7919 - mse: 8.7919 - mae: 2.2181\n",
      "Epoch 62/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 8.7168 - mse: 8.7168 - mae: 2.1929\n",
      "Epoch 63/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 8.6926 - mse: 8.6926 - mae: 2.1790\n",
      "Epoch 64/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 8.6684 - mse: 8.6684 - mae: 2.2005\n",
      "Epoch 65/100\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 8.5740 - mse: 8.5740 - mae: 2.1654\n",
      "Epoch 66/100\n",
      "392/392 [==============================] - 0s 84us/sample - loss: 8.5282 - mse: 8.5282 - mae: 2.1560\n",
      "Epoch 67/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 8.4092 - mse: 8.4092 - mae: 2.1563\n",
      "Epoch 68/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 8.3844 - mse: 8.3844 - mae: 2.1381\n",
      "Epoch 69/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 8.3204 - mse: 8.3204 - mae: 2.1381\n",
      "Epoch 70/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 8.2447 - mse: 8.2447 - mae: 2.1192\n",
      "Epoch 71/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 8.3423 - mse: 8.3423 - mae: 2.1088\n",
      "Epoch 72/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 8.3975 - mse: 8.3975 - mae: 2.1524\n",
      "Epoch 73/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 8.3488 - mse: 8.3488 - mae: 2.1133\n",
      "Epoch 74/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 8.1306 - mse: 8.1306 - mae: 2.0937\n",
      "Epoch 75/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 8.1159 - mse: 8.1159 - mae: 2.0879\n",
      "Epoch 76/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 8.0197 - mse: 8.0197 - mae: 2.0761\n",
      "Epoch 77/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 7.9882 - mse: 7.9882 - mae: 2.0719\n",
      "Epoch 78/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 7.9456 - mse: 7.9456 - mae: 2.0578\n",
      "Epoch 79/100\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 7.9358 - mse: 7.9358 - mae: 2.0686\n",
      "Epoch 80/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 8.0136 - mse: 8.0136 - mae: 2.0554\n",
      "Epoch 81/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 8.1065 - mse: 8.1065 - mae: 2.1014\n",
      "Epoch 82/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 8.0684 - mse: 8.0684 - mae: 2.0569\n",
      "Epoch 83/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 7.7943 - mse: 7.7943 - mae: 2.0417\n",
      "Epoch 84/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 7.8039 - mse: 7.8039 - mae: 2.0420\n",
      "Epoch 85/100\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 7.7596 - mse: 7.7596 - mae: 2.0264\n",
      "Epoch 86/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 7.7451 - mse: 7.7451 - mae: 2.0262\n",
      "Epoch 87/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 7.7634 - mse: 7.7634 - mae: 2.0136\n",
      "Epoch 88/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 7.7540 - mse: 7.7540 - mae: 2.0325\n",
      "Epoch 89/100\n",
      "392/392 [==============================] - 0s 54us/sample - loss: 7.6931 - mse: 7.6931 - mae: 2.0156\n",
      "Epoch 90/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 7.6147 - mse: 7.6147 - mae: 2.0041\n",
      "Epoch 91/100\n",
      "392/392 [==============================] - 0s 59us/sample - loss: 7.6519 - mse: 7.6519 - mae: 2.0095\n",
      "Epoch 92/100\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 7.6437 - mse: 7.6437 - mae: 2.0031\n",
      "Epoch 93/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 7.6067 - mse: 7.6067 - mae: 1.9946\n",
      "Epoch 94/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 7.5879 - mse: 7.5879 - mae: 1.9924\n",
      "Epoch 95/100\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 7.5525 - mse: 7.5525 - mae: 1.9876\n",
      "Epoch 96/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 7.5425 - mse: 7.5425 - mae: 1.9889\n",
      "Epoch 97/100\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 7.5894 - mse: 7.5894 - mae: 1.9953\n",
      "Epoch 98/100\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 7.5057 - mse: 7.5057 - mae: 1.9873\n",
      "Epoch 99/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 7.5508 - mse: 7.5508 - mae: 1.9859\n",
      "Epoch 100/100\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 7.4682 - mse: 7.4682 - mae: 1.9829\n"
     ]
    }
   ],
   "source": [
    "# fit할 때 return 값이 있다.\n",
    "# hh에 할당해본다.\n",
    "hh = model_oh_mm.fit(X_oh, mpg_pd[0].values, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능 : loss: 7.4682 - mse: 7.4682 - mae: 1.9829"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_chief_worker_only',\n",
       " '_keras_api_names',\n",
       " '_keras_api_names_v1',\n",
       " 'epoch',\n",
       " 'history',\n",
       " 'model',\n",
       " 'on_batch_begin',\n",
       " 'on_batch_end',\n",
       " 'on_epoch_begin',\n",
       " 'on_epoch_end',\n",
       " 'on_predict_batch_begin',\n",
       " 'on_predict_batch_end',\n",
       " 'on_predict_begin',\n",
       " 'on_predict_end',\n",
       " 'on_test_batch_begin',\n",
       " 'on_test_batch_end',\n",
       " 'on_test_begin',\n",
       " 'on_test_end',\n",
       " 'on_train_batch_begin',\n",
       " 'on_train_batch_end',\n",
       " 'on_train_begin',\n",
       " 'on_train_end',\n",
       " 'params',\n",
       " 'set_model',\n",
       " 'set_params',\n",
       " 'validation_data']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습할 때 반환되는 값이 무엇인가 살펴보자.\n",
    "# history가 거슬린다.\n",
    "dir(hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [587.4661142777423,\n",
       "  550.7234621631856,\n",
       "  498.5111168063417,\n",
       "  421.4042445591518,\n",
       "  320.12331468231827,\n",
       "  214.98248571279098,\n",
       "  146.53756651586417,\n",
       "  114.0858453244579,\n",
       "  91.97474701550541,\n",
       "  72.07812141885563,\n",
       "  56.06037505792112,\n",
       "  44.92035924171915,\n",
       "  36.09435544695173,\n",
       "  30.526237954898757,\n",
       "  26.30788951017419,\n",
       "  23.329634841607543,\n",
       "  21.51971233134367,\n",
       "  19.751751004433146,\n",
       "  18.54521116918447,\n",
       "  17.624100899209783,\n",
       "  16.789563237404337,\n",
       "  16.169457610772582,\n",
       "  15.610112939562116,\n",
       "  15.137521237743144,\n",
       "  14.689472665592115,\n",
       "  14.408105188486527,\n",
       "  13.99164188151457,\n",
       "  13.584694803977499,\n",
       "  13.475020291853925,\n",
       "  12.980298100685587,\n",
       "  12.74876594543457,\n",
       "  12.52428490774972,\n",
       "  12.30680021947744,\n",
       "  12.103847620438556,\n",
       "  11.851295159787547,\n",
       "  11.705896494339923,\n",
       "  11.613963312032272,\n",
       "  11.338732349629305,\n",
       "  11.257079289883983,\n",
       "  10.97133478826406,\n",
       "  10.938033979766223,\n",
       "  10.806583754870356,\n",
       "  10.629924890946368,\n",
       "  10.448162662739657,\n",
       "  10.309110407926598,\n",
       "  10.196329262791847,\n",
       "  10.20104593160201,\n",
       "  9.90490368434361,\n",
       "  9.83789181222721,\n",
       "  9.694940654598938,\n",
       "  9.622135629459303,\n",
       "  9.569132824333346,\n",
       "  9.489409174237933,\n",
       "  9.33278591292245,\n",
       "  9.25912179752272,\n",
       "  9.21683457432961,\n",
       "  9.103229172375737,\n",
       "  9.011415578881088,\n",
       "  8.90817388223142,\n",
       "  8.85566793169294,\n",
       "  8.791901919306541,\n",
       "  8.716798208197769,\n",
       "  8.692634154339226,\n",
       "  8.668426163342534,\n",
       "  8.574030837234186,\n",
       "  8.528207233973912,\n",
       "  8.409222675829518,\n",
       "  8.384402489175601,\n",
       "  8.320352495933065,\n",
       "  8.244746003832136,\n",
       "  8.342306039771255,\n",
       "  8.397508407125668,\n",
       "  8.348837249133052,\n",
       "  8.130560232668508,\n",
       "  8.115926859330157,\n",
       "  8.019660521526726,\n",
       "  7.988193375723703,\n",
       "  7.9456363989382375,\n",
       "  7.93578104097016,\n",
       "  8.013634044296888,\n",
       "  8.106511914000219,\n",
       "  8.068369547931516,\n",
       "  7.794284742705676,\n",
       "  7.803915585790362,\n",
       "  7.759618175273039,\n",
       "  7.745121371989348,\n",
       "  7.763403620038714,\n",
       "  7.754014268213389,\n",
       "  7.693144396859772,\n",
       "  7.614713590972277,\n",
       "  7.651902072283686,\n",
       "  7.643698264141472,\n",
       "  7.606738440844477,\n",
       "  7.587904229456065,\n",
       "  7.5525252283835895,\n",
       "  7.542546466905243,\n",
       "  7.589364085878644,\n",
       "  7.505681152246436,\n",
       "  7.550830675631153,\n",
       "  7.468198523229482],\n",
       " 'mse': [587.46606,\n",
       "  550.72345,\n",
       "  498.51117,\n",
       "  421.4043,\n",
       "  320.12335,\n",
       "  214.9825,\n",
       "  146.53755,\n",
       "  114.085846,\n",
       "  91.97476,\n",
       "  72.078125,\n",
       "  56.06038,\n",
       "  44.92036,\n",
       "  36.094357,\n",
       "  30.526237,\n",
       "  26.307888,\n",
       "  23.329636,\n",
       "  21.51971,\n",
       "  19.75175,\n",
       "  18.545212,\n",
       "  17.6241,\n",
       "  16.789564,\n",
       "  16.169456,\n",
       "  15.610114,\n",
       "  15.13752,\n",
       "  14.689472,\n",
       "  14.408105,\n",
       "  13.991643,\n",
       "  13.584694,\n",
       "  13.475021,\n",
       "  12.980299,\n",
       "  12.748766,\n",
       "  12.524284,\n",
       "  12.3068,\n",
       "  12.103848,\n",
       "  11.8512945,\n",
       "  11.705896,\n",
       "  11.613962,\n",
       "  11.338733,\n",
       "  11.257079,\n",
       "  10.971334,\n",
       "  10.938033,\n",
       "  10.806582,\n",
       "  10.629925,\n",
       "  10.448163,\n",
       "  10.30911,\n",
       "  10.196329,\n",
       "  10.201046,\n",
       "  9.904903,\n",
       "  9.837892,\n",
       "  9.69494,\n",
       "  9.622136,\n",
       "  9.569132,\n",
       "  9.4894085,\n",
       "  9.332787,\n",
       "  9.259122,\n",
       "  9.216834,\n",
       "  9.1032295,\n",
       "  9.0114155,\n",
       "  8.908174,\n",
       "  8.855668,\n",
       "  8.791903,\n",
       "  8.716798,\n",
       "  8.692634,\n",
       "  8.668426,\n",
       "  8.574031,\n",
       "  8.528207,\n",
       "  8.409223,\n",
       "  8.384402,\n",
       "  8.320353,\n",
       "  8.244746,\n",
       "  8.342306,\n",
       "  8.397509,\n",
       "  8.348839,\n",
       "  8.130561,\n",
       "  8.115928,\n",
       "  8.019661,\n",
       "  7.9881935,\n",
       "  7.945636,\n",
       "  7.9357805,\n",
       "  8.013634,\n",
       "  8.106513,\n",
       "  8.068369,\n",
       "  7.7942853,\n",
       "  7.803916,\n",
       "  7.759618,\n",
       "  7.745121,\n",
       "  7.763404,\n",
       "  7.7540145,\n",
       "  7.6931443,\n",
       "  7.614713,\n",
       "  7.651902,\n",
       "  7.6436987,\n",
       "  7.606738,\n",
       "  7.587905,\n",
       "  7.5525255,\n",
       "  7.542547,\n",
       "  7.5893636,\n",
       "  7.505681,\n",
       "  7.5508304,\n",
       "  7.468199],\n",
       " 'mae': [22.926146,\n",
       "  22.078785,\n",
       "  20.765612,\n",
       "  18.646511,\n",
       "  15.437199,\n",
       "  11.846955,\n",
       "  10.260646,\n",
       "  9.352569,\n",
       "  8.397576,\n",
       "  7.364426,\n",
       "  6.4323115,\n",
       "  5.6638794,\n",
       "  5.009698,\n",
       "  4.5487328,\n",
       "  4.150572,\n",
       "  3.856125,\n",
       "  3.6764905,\n",
       "  3.4797046,\n",
       "  3.347406,\n",
       "  3.2571886,\n",
       "  3.1760714,\n",
       "  3.1228096,\n",
       "  3.074055,\n",
       "  3.0298617,\n",
       "  2.986686,\n",
       "  2.956064,\n",
       "  2.9171438,\n",
       "  2.8716555,\n",
       "  2.8362966,\n",
       "  2.8019125,\n",
       "  2.772951,\n",
       "  2.7437274,\n",
       "  2.720982,\n",
       "  2.6868496,\n",
       "  2.6655915,\n",
       "  2.6446383,\n",
       "  2.6378012,\n",
       "  2.5943685,\n",
       "  2.5708966,\n",
       "  2.5415905,\n",
       "  2.5483563,\n",
       "  2.5205445,\n",
       "  2.4959571,\n",
       "  2.470115,\n",
       "  2.447345,\n",
       "  2.439992,\n",
       "  2.4274523,\n",
       "  2.3842506,\n",
       "  2.366348,\n",
       "  2.3525412,\n",
       "  2.3430312,\n",
       "  2.3380778,\n",
       "  2.3064606,\n",
       "  2.2993305,\n",
       "  2.2869184,\n",
       "  2.280267,\n",
       "  2.2525127,\n",
       "  2.2353382,\n",
       "  2.2217646,\n",
       "  2.2186067,\n",
       "  2.2181318,\n",
       "  2.192896,\n",
       "  2.1790197,\n",
       "  2.200456,\n",
       "  2.1654475,\n",
       "  2.1559777,\n",
       "  2.156345,\n",
       "  2.1380847,\n",
       "  2.1381402,\n",
       "  2.1191998,\n",
       "  2.1087794,\n",
       "  2.1524112,\n",
       "  2.1132796,\n",
       "  2.093664,\n",
       "  2.0878801,\n",
       "  2.0760765,\n",
       "  2.0719151,\n",
       "  2.057844,\n",
       "  2.06857,\n",
       "  2.055377,\n",
       "  2.1014087,\n",
       "  2.0568976,\n",
       "  2.0416985,\n",
       "  2.04202,\n",
       "  2.0264113,\n",
       "  2.0262263,\n",
       "  2.0136056,\n",
       "  2.0325031,\n",
       "  2.0156205,\n",
       "  2.0040832,\n",
       "  2.009467,\n",
       "  2.00315,\n",
       "  1.9945532,\n",
       "  1.9923762,\n",
       "  1.9876465,\n",
       "  1.9889231,\n",
       "  1.9953252,\n",
       "  1.9872675,\n",
       "  1.9858551,\n",
       "  1.9828793]}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>587.466114</td>\n",
       "      <td>587.466064</td>\n",
       "      <td>22.926146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>550.723462</td>\n",
       "      <td>550.723450</td>\n",
       "      <td>22.078785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>498.511117</td>\n",
       "      <td>498.511169</td>\n",
       "      <td>20.765612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>421.404245</td>\n",
       "      <td>421.404297</td>\n",
       "      <td>18.646511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>320.123315</td>\n",
       "      <td>320.123352</td>\n",
       "      <td>15.437199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>7.542546</td>\n",
       "      <td>7.542547</td>\n",
       "      <td>1.988923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>7.589364</td>\n",
       "      <td>7.589364</td>\n",
       "      <td>1.995325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>7.505681</td>\n",
       "      <td>7.505681</td>\n",
       "      <td>1.987267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>7.550831</td>\n",
       "      <td>7.550830</td>\n",
       "      <td>1.985855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>7.468199</td>\n",
       "      <td>7.468199</td>\n",
       "      <td>1.982879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss         mse        mae\n",
       "0   587.466114  587.466064  22.926146\n",
       "1   550.723462  550.723450  22.078785\n",
       "2   498.511117  498.511169  20.765612\n",
       "3   421.404245  421.404297  18.646511\n",
       "4   320.123315  320.123352  15.437199\n",
       "..         ...         ...        ...\n",
       "95    7.542546    7.542547   1.988923\n",
       "96    7.589364    7.589364   1.995325\n",
       "97    7.505681    7.505681   1.987267\n",
       "98    7.550831    7.550830   1.985855\n",
       "99    7.468199    7.468199   1.982879\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas로 만들 수 있다.\n",
    "pd.DataFrame(hh.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x29b49fc8c88>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pandas의 장점 : 시각화가 편리하다.\n",
    "# learning curve\n",
    "pd.DataFrame(hh.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_keras_api_names',\n",
       " '_keras_api_names_v1',\n",
       " 'on_batch_begin',\n",
       " 'on_batch_end',\n",
       " 'on_epoch_begin',\n",
       " 'on_epoch_end',\n",
       " 'on_predict_batch_begin',\n",
       " 'on_predict_batch_end',\n",
       " 'on_predict_begin',\n",
       " 'on_predict_end',\n",
       " 'on_test_batch_begin',\n",
       " 'on_test_batch_end',\n",
       " 'on_test_begin',\n",
       " 'on_test_end',\n",
       " 'on_train_batch_begin',\n",
       " 'on_train_batch_end',\n",
       " 'on_train_begin',\n",
       " 'on_train_end',\n",
       " 'set_model',\n",
       " 'set_params']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(Callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MyCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.keras.callbacks.Callback.on_epoch_begin(self, epoch, logs=None)>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Callback.on_epoch_begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상속 받아서 내가 원하는 기능 추가한다.\n",
    "# overriding : 부모가 가진 메소드 이름 똑같이 해서 덮어쓴다.\n",
    "class MyCallback(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None): # 인자도 부모와 똑같이 만든다.\n",
    "        print('epoch 시작')\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('epoch 끝')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Train on 392 samples\n",
      "epoch 시작\n",
      "Epoch 1/100\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_initialize_variables_142897 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_143120 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " 32/392 [=>............................] - ETA: 1:04 - loss: 581.6897 - mse: 581.6897 - mae: 22.6667epoch 끝\n",
      "392/392 [==============================] - 6s 15ms/sample - loss: 598.9095 - mse: 598.9095 - mae: 23.1880\n",
      "epoch 시작\n",
      "Epoch 2/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 522.6238 - mse: 522.6238 - mae: 21.2738epoch 끝\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 569.1490 - mse: 569.1490 - mae: 22.5344\n",
      "epoch 시작\n",
      "Epoch 3/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 503.3343 - mse: 503.3343 - mae: 21.2528epoch 끝\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 526.9306 - mse: 526.9305 - mae: 21.5463\n",
      "epoch 시작\n",
      "Epoch 4/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 453.0394 - mse: 453.0394 - mae: 20.1531epoch 끝\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 459.4379 - mse: 459.4379 - mae: 19.8389\n",
      "epoch 시작\n",
      "Epoch 5/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 364.3445 - mse: 364.3445 - mae: 17.6236epoch 끝\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 361.4950 - mse: 361.4950 - mae: 17.0383\n",
      "epoch 시작\n",
      "Epoch 6/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 242.8985 - mse: 242.8985 - mae: 13.3083epoch 끝\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 243.2495 - mse: 243.2495 - mae: 13.1038\n",
      "epoch 시작\n",
      "Epoch 7/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 197.1913 - mse: 197.1913 - mae: 11.6546epoch 끝\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 143.4439 - mse: 143.4440 - mae: 9.8594\n",
      "epoch 시작\n",
      "Epoch 8/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 114.3601 - mse: 114.3601 - mae: 9.3812epoch 끝\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 98.0780 - mse: 98.0780 - mae: 8.3883\n",
      "epoch 시작\n",
      "Epoch 9/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 87.3652 - mse: 87.3652 - mae: 7.7690epoch 끝\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 80.8568 - mse: 80.8568 - mae: 7.6109\n",
      "epoch 시작\n",
      "Epoch 10/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 73.3824 - mse: 73.3824 - mae: 7.4392epoch 끝\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 65.0676 - mse: 65.0676 - mae: 6.7509\n",
      "epoch 시작\n",
      "Epoch 11/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 63.1598 - mse: 63.1598 - mae: 6.7136epoch 끝\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 53.6701 - mse: 53.6701 - mae: 6.0281\n",
      "epoch 시작\n",
      "Epoch 12/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 59.0693 - mse: 59.0693 - mae: 6.2679epoch 끝\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 45.0687 - mse: 45.0687 - mae: 5.4315\n",
      "epoch 시작\n",
      "Epoch 13/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 30.9494 - mse: 30.9494 - mae: 4.7590epoch 끝\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 38.8424 - mse: 38.8424 - mae: 5.0337\n",
      "epoch 시작\n",
      "Epoch 14/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 33.9716 - mse: 33.9716 - mae: 4.5201epoch 끝\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 34.4614 - mse: 34.4614 - mae: 4.6982\n",
      "epoch 시작\n",
      "Epoch 15/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 41.0008 - mse: 41.0008 - mae: 5.2656epoch 끝\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 31.3552 - mse: 31.3552 - mae: 4.4352\n",
      "epoch 시작\n",
      "Epoch 16/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 27.0489 - mse: 27.0489 - mae: 4.3410epoch 끝\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 28.9025 - mse: 28.9025 - mae: 4.1712\n",
      "epoch 시작\n",
      "Epoch 17/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 32.0101 - mse: 32.0101 - mae: 4.0835epoch 끝\n",
      "392/392 [==============================] - 0s 61us/sample - loss: 27.0113 - mse: 27.0113 - mae: 3.9919\n",
      "epoch 시작\n",
      "Epoch 18/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 18.9309 - mse: 18.9309 - mae: 3.3827epoch 끝\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 25.2808 - mse: 25.2808 - mae: 3.8769\n",
      "epoch 시작\n",
      "Epoch 19/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 26.9378 - mse: 26.9378 - mae: 4.0807epoch 끝\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 23.9739 - mse: 23.9739 - mae: 3.7621\n",
      "epoch 시작\n",
      "Epoch 20/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 9.9885 - mse: 9.9885 - mae: 2.3714epoch 끝\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 22.8568 - mse: 22.8568 - mae: 3.6675\n",
      "epoch 시작\n",
      "Epoch 21/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 18.5227 - mse: 18.5227 - mae: 3.1987epoch 끝\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 21.8670 - mse: 21.8670 - mae: 3.5901\n",
      "epoch 시작\n",
      "Epoch 22/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 25.3989 - mse: 25.3989 - mae: 3.8923epoch 끝\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 21.0622 - mse: 21.0622 - mae: 3.5322\n",
      "epoch 시작\n",
      "Epoch 23/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 24.2169 - mse: 24.2169 - mae: 3.6290epoch 끝\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 20.2010 - mse: 20.2010 - mae: 3.4560\n",
      "epoch 시작\n",
      "Epoch 24/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 15.6378 - mse: 15.6378 - mae: 2.8838epoch 끝\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 19.4525 - mse: 19.4525 - mae: 3.3913\n",
      "epoch 시작\n",
      "Epoch 25/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 23.2873 - mse: 23.2873 - mae: 3.7576epoch 끝\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 18.7518 - mse: 18.7518 - mae: 3.3514\n",
      "epoch 시작\n",
      "Epoch 26/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 19.2331 - mse: 19.2331 - mae: 3.5508epoch 끝\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 18.1577 - mse: 18.1577 - mae: 3.3069\n",
      "epoch 시작\n",
      "Epoch 27/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 13.0500 - mse: 13.0500 - mae: 2.9383epoch 끝\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 17.7305 - mse: 17.7305 - mae: 3.2865\n",
      "epoch 시작\n",
      "Epoch 28/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 11.5092 - mse: 11.5092 - mae: 2.6114epoch 끝\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 17.2382 - mse: 17.2382 - mae: 3.2278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 시작\n",
      "Epoch 29/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 18.5419 - mse: 18.5419 - mae: 3.2730epoch 끝\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 16.7627 - mse: 16.7627 - mae: 3.2005\n",
      "epoch 시작\n",
      "Epoch 30/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 12.2153 - mse: 12.2153 - mae: 2.6461epoch 끝\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 16.2873 - mse: 16.2873 - mae: 3.1510\n",
      "epoch 시작\n",
      "Epoch 31/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 17.3777 - mse: 17.3777 - mae: 3.3716epoch 끝\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 15.9045 - mse: 15.9045 - mae: 3.1059\n",
      "epoch 시작\n",
      "Epoch 32/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 9.5904 - mse: 9.5904 - mae: 2.4471epoch 끝\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 15.5945 - mse: 15.5945 - mae: 3.0984\n",
      "epoch 시작\n",
      "Epoch 33/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 23.0623 - mse: 23.0623 - mae: 3.8485epoch 끝\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 15.2638 - mse: 15.2638 - mae: 3.0654\n",
      "epoch 시작\n",
      "Epoch 34/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 20.4989 - mse: 20.4989 - mae: 3.3795epoch 끝\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 14.8273 - mse: 14.8273 - mae: 3.0167\n",
      "epoch 시작\n",
      "Epoch 35/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 9.8126 - mse: 9.8126 - mae: 2.4724epoch 끝\n",
      "392/392 [==============================] - 0s 87us/sample - loss: 14.5099 - mse: 14.5099 - mae: 2.9920\n",
      "epoch 시작\n",
      "Epoch 36/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 18.6425 - mse: 18.6425 - mae: 3.3753epoch 끝\n",
      "392/392 [==============================] - 0s 76us/sample - loss: 14.2072 - mse: 14.2072 - mae: 2.9540\n",
      "epoch 시작\n",
      "Epoch 37/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 14.8784 - mse: 14.8784 - mae: 3.0876epoch 끝\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 13.9438 - mse: 13.9438 - mae: 2.9226\n",
      "epoch 시작\n",
      "Epoch 38/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 12.7175 - mse: 12.7175 - mae: 2.8164epoch 끝\n",
      "392/392 [==============================] - 0s 87us/sample - loss: 13.7386 - mse: 13.7386 - mae: 2.9019\n",
      "epoch 시작\n",
      "Epoch 39/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 23.7817 - mse: 23.7817 - mae: 3.7741epoch 끝\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 13.3923 - mse: 13.3923 - mae: 2.8562\n",
      "epoch 시작\n",
      "Epoch 40/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 8.0881 - mse: 8.0881 - mae: 2.2220epoch 끝\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 13.1545 - mse: 13.1545 - mae: 2.8279\n",
      "epoch 시작\n",
      "Epoch 41/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 8.9247 - mse: 8.9247 - mae: 2.5363epoch 끝\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 12.9517 - mse: 12.9517 - mae: 2.7965\n",
      "epoch 시작\n",
      "Epoch 42/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 14.2201 - mse: 14.2201 - mae: 2.6785epoch 끝\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 12.8013 - mse: 12.8013 - mae: 2.7865\n",
      "epoch 시작\n",
      "Epoch 43/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 13.1501 - mse: 13.1501 - mae: 3.0680epoch 끝\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 12.6701 - mse: 12.6701 - mae: 2.7648\n",
      "epoch 시작\n",
      "Epoch 44/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 12.8277 - mse: 12.8277 - mae: 2.9516epoch 끝\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 12.1729 - mse: 12.1729 - mae: 2.7096\n",
      "epoch 시작\n",
      "Epoch 45/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 11.0295 - mse: 11.0295 - mae: 2.4270epoch 끝\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 11.9876 - mse: 11.9876 - mae: 2.6872\n",
      "epoch 시작\n",
      "Epoch 46/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 14.3539 - mse: 14.3539 - mae: 2.6912epoch 끝\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 11.7549 - mse: 11.7549 - mae: 2.6572\n",
      "epoch 시작\n",
      "Epoch 47/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 11.6991 - mse: 11.6991 - mae: 2.7760epoch 끝\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 11.5442 - mse: 11.5442 - mae: 2.6169\n",
      "epoch 시작\n",
      "Epoch 48/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 8.9389 - mse: 8.9389 - mae: 2.2088epoch 끝\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 11.3131 - mse: 11.3131 - mae: 2.5895\n",
      "epoch 시작\n",
      "Epoch 49/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 5.4054 - mse: 5.4054 - mae: 1.8259epoch 끝\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 11.2018 - mse: 11.2018 - mae: 2.5619\n",
      "epoch 시작\n",
      "Epoch 50/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 14.2296 - mse: 14.2296 - mae: 2.9041epoch 끝\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 11.0636 - mse: 11.0636 - mae: 2.5547\n",
      "epoch 시작\n",
      "Epoch 51/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 14.9235 - mse: 14.9235 - mae: 2.6846epoch 끝\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 10.9011 - mse: 10.9011 - mae: 2.5255\n",
      "epoch 시작\n",
      "Epoch 52/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 17.7435 - mse: 17.7435 - mae: 3.3216epoch 끝\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 10.6212 - mse: 10.6212 - mae: 2.4846\n",
      "epoch 시작\n",
      "Epoch 53/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 9.9324 - mse: 9.9324 - mae: 2.5099epoch 끝\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 10.3903 - mse: 10.3903 - mae: 2.4532\n",
      "epoch 시작\n",
      "Epoch 54/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 9.8596 - mse: 9.8596 - mae: 2.5182epoch 끝\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 10.2227 - mse: 10.2227 - mae: 2.4252\n",
      "epoch 시작\n",
      "Epoch 55/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 9.3878 - mse: 9.3878 - mae: 2.5539epoch 끝\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 10.0768 - mse: 10.0768 - mae: 2.4013\n",
      "epoch 시작\n",
      "Epoch 56/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 7.3403 - mse: 7.3403 - mae: 1.9174epoch 끝\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 9.9730 - mse: 9.9730 - mae: 2.3910\n",
      "epoch 시작\n",
      "Epoch 57/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 10.8757 - mse: 10.8757 - mae: 2.2870epoch 끝\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 9.7952 - mse: 9.7952 - mae: 2.3604\n",
      "epoch 시작\n",
      "Epoch 58/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 13.0854 - mse: 13.0854 - mae: 2.6579epoch 끝\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 9.6732 - mse: 9.6732 - mae: 2.3364\n",
      "epoch 시작\n",
      "Epoch 59/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 13.8438 - mse: 13.8438 - mae: 2.6821epoch 끝\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 9.5986 - mse: 9.5986 - mae: 2.3376\n",
      "epoch 시작\n",
      "Epoch 60/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 7.2620 - mse: 7.2620 - mae: 2.1538epoch 끝\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 9.6197 - mse: 9.6197 - mae: 2.3088\n",
      "epoch 시작\n",
      "Epoch 61/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 11.7259 - mse: 11.7259 - mae: 2.3321epoch 끝\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 9.6694 - mse: 9.6694 - mae: 2.3542\n",
      "epoch 시작\n",
      "Epoch 62/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 8.0835 - mse: 8.0835 - mae: 2.1278epoch 끝\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 9.3967 - mse: 9.3967 - mae: 2.2783\n",
      "epoch 시작\n",
      "Epoch 63/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 10.7447 - mse: 10.7447 - mae: 2.4234epoch 끝\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 9.0030 - mse: 9.0030 - mae: 2.2320\n",
      "epoch 시작\n",
      "Epoch 64/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 9.3260 - mse: 9.3260 - mae: 2.5584epoch 끝\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 9.0427 - mse: 9.0427 - mae: 2.2451\n",
      "epoch 시작\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/392 [=>............................] - ETA: 0s - loss: 13.3315 - mse: 13.3315 - mae: 2.6379epoch 끝\n",
      "392/392 [==============================] - 0s 64us/sample - loss: 8.9605 - mse: 8.9605 - mae: 2.2165\n",
      "epoch 시작\n",
      "Epoch 66/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 13.1084 - mse: 13.1084 - mae: 2.7887epoch 끝\n",
      "392/392 [==============================] - 0s 66us/sample - loss: 8.9470 - mse: 8.9470 - mae: 2.2265\n",
      "epoch 시작\n",
      "Epoch 67/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 10.4521 - mse: 10.4521 - mae: 2.0185epoch 끝\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 8.9611 - mse: 8.9611 - mae: 2.2195\n",
      "epoch 시작\n",
      "Epoch 68/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 8.0727 - mse: 8.0727 - mae: 2.1850epoch 끝\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 8.7908 - mse: 8.7908 - mae: 2.1954\n",
      "epoch 시작\n",
      "Epoch 69/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 6.9935 - mse: 6.9935 - mae: 2.1183epoch 끝\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 8.5521 - mse: 8.5521 - mae: 2.1651\n",
      "epoch 시작\n",
      "Epoch 70/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 13.6948 - mse: 13.6948 - mae: 2.9034epoch 끝\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 8.6869 - mse: 8.6869 - mae: 2.1779\n",
      "epoch 시작\n",
      "Epoch 71/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 14.2572 - mse: 14.2572 - mae: 2.8067epoch 끝\n",
      "392/392 [==============================] - 0s 71us/sample - loss: 8.5291 - mse: 8.5291 - mae: 2.1593\n",
      "epoch 시작\n",
      "Epoch 72/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 5.7704 - mse: 5.7704 - mae: 1.9022epoch 끝\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 8.5692 - mse: 8.5692 - mae: 2.1506\n",
      "epoch 시작\n",
      "Epoch 73/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 14.6716 - mse: 14.6716 - mae: 2.9478epoch 끝\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 8.4379 - mse: 8.4379 - mae: 2.1404\n",
      "epoch 시작\n",
      "Epoch 74/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 7.5866 - mse: 7.5866 - mae: 2.0522epoch 끝\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 8.3429 - mse: 8.3429 - mae: 2.1249\n",
      "epoch 시작\n",
      "Epoch 75/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 10.9712 - mse: 10.9712 - mae: 2.2869epoch 끝\n",
      "392/392 [==============================] - 0s 69us/sample - loss: 8.3161 - mse: 8.3161 - mae: 2.1150\n",
      "epoch 시작\n",
      "Epoch 76/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 13.2595 - mse: 13.2595 - mae: 2.4432epoch 끝\n",
      "392/392 [==============================] - 0s 87us/sample - loss: 8.2815 - mse: 8.2815 - mae: 2.1111\n",
      "epoch 시작\n",
      "Epoch 77/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 10.8619 - mse: 10.8619 - mae: 2.5601epoch 끝\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 8.2854 - mse: 8.2854 - mae: 2.1044\n",
      "epoch 시작\n",
      "Epoch 78/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 6.2732 - mse: 6.2732 - mae: 1.7178epoch 끝\n",
      "392/392 [==============================] - 0s 79us/sample - loss: 8.2986 - mse: 8.2986 - mae: 2.1235\n",
      "epoch 시작\n",
      "Epoch 79/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 9.5897 - mse: 9.5897 - mae: 2.0372epoch 끝\n",
      "392/392 [==============================] - 0s 82us/sample - loss: 8.1856 - mse: 8.1856 - mae: 2.0914\n",
      "epoch 시작\n",
      "Epoch 80/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 6.7497 - mse: 6.7497 - mae: 1.9508epoch 끝\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 8.1422 - mse: 8.1422 - mae: 2.0910\n",
      "epoch 시작\n",
      "Epoch 81/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 5.5854 - mse: 5.5854 - mae: 1.8807epoch 끝\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 8.1885 - mse: 8.1885 - mae: 2.0824\n",
      "epoch 시작\n",
      "Epoch 82/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 5.9591 - mse: 5.9591 - mae: 1.9472epoch 끝\n",
      "392/392 [==============================] - 0s 89us/sample - loss: 8.7154 - mse: 8.7154 - mae: 2.2016\n",
      "epoch 시작\n",
      "Epoch 83/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 11.4456 - mse: 11.4456 - mae: 2.7929epoch 끝\n",
      "392/392 [==============================] - 0s 74us/sample - loss: 8.2152 - mse: 8.2152 - mae: 2.1007\n",
      "epoch 시작\n",
      "Epoch 84/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 9.8155 - mse: 9.8155 - mae: 2.1046epoch 끝\n",
      "392/392 [==============================] - 0s 87us/sample - loss: 8.1531 - mse: 8.1531 - mae: 2.0857\n",
      "epoch 시작\n",
      "Epoch 85/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 7.1968 - mse: 7.1968 - mae: 2.1108epoch 끝\n",
      "392/392 [==============================] - 0s 97us/sample - loss: 8.0793 - mse: 8.0793 - mae: 2.0605\n",
      "epoch 시작\n",
      "Epoch 86/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 6.0402 - mse: 6.0402 - mae: 1.6094epoch 끝\n",
      "392/392 [==============================] - 0s 87us/sample - loss: 7.9864 - mse: 7.9864 - mae: 2.0735\n",
      "epoch 시작\n",
      "Epoch 87/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 11.7331 - mse: 11.7331 - mae: 2.4926epoch 끝\n",
      "392/392 [==============================] - 0s 89us/sample - loss: 7.9135 - mse: 7.9135 - mae: 2.0465\n",
      "epoch 시작\n",
      "Epoch 88/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 8.3471 - mse: 8.3471 - mae: 1.8955epoch 끝\n",
      "392/392 [==============================] - 0s 77us/sample - loss: 7.9428 - mse: 7.9428 - mae: 2.0558\n",
      "epoch 시작\n",
      "Epoch 89/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 9.4823 - mse: 9.4823 - mae: 2.2759epoch 끝\n",
      "392/392 [==============================] - 0s 92us/sample - loss: 7.9132 - mse: 7.9132 - mae: 2.0509\n",
      "epoch 시작\n",
      "Epoch 90/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 6.9557 - mse: 6.9557 - mae: 2.2392epoch 끝\n",
      "392/392 [==============================] - 0s 125us/sample - loss: 7.9529 - mse: 7.9529 - mae: 2.0438\n",
      "epoch 시작\n",
      "Epoch 91/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 10.8931 - mse: 10.8931 - mae: 2.3800epoch 끝\n",
      "392/392 [==============================] - 0s 110us/sample - loss: 7.9503 - mse: 7.9503 - mae: 2.0640\n",
      "epoch 시작\n",
      "Epoch 92/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 6.1157 - mse: 6.1157 - mae: 1.9136epoch 끝\n",
      "392/392 [==============================] - 0s 97us/sample - loss: 7.9663 - mse: 7.9663 - mae: 2.0439\n",
      "epoch 시작\n",
      "Epoch 93/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 5.8729 - mse: 5.8729 - mae: 1.5989epoch 끝\n",
      "392/392 [==============================] - 0s 107us/sample - loss: 7.8702 - mse: 7.8702 - mae: 2.0462\n",
      "epoch 시작\n",
      "Epoch 94/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 5.9784 - mse: 5.9784 - mae: 1.7846epoch 끝\n",
      "392/392 [==============================] - 0s 99us/sample - loss: 7.8248 - mse: 7.8248 - mae: 2.0230\n",
      "epoch 시작\n",
      "Epoch 95/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 7.1401 - mse: 7.1401 - mae: 1.9990epoch 끝\n",
      "392/392 [==============================] - 0s 99us/sample - loss: 7.7607 - mse: 7.7607 - mae: 2.0250\n",
      "epoch 시작\n",
      "Epoch 96/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 3.6749 - mse: 3.6749 - mae: 1.5979epoch 끝\n",
      "392/392 [==============================] - 0s 97us/sample - loss: 7.7680 - mse: 7.7680 - mae: 2.0290\n",
      "epoch 시작\n",
      "Epoch 97/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 11.4903 - mse: 11.4903 - mae: 2.5306epoch 끝\n",
      "392/392 [==============================] - 0s 94us/sample - loss: 7.7564 - mse: 7.7564 - mae: 2.0180\n",
      "epoch 시작\n",
      "Epoch 98/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 9.6701 - mse: 9.6701 - mae: 2.1049epoch 끝\n",
      "392/392 [==============================] - 0s 107us/sample - loss: 7.7145 - mse: 7.7145 - mae: 2.0146\n",
      "epoch 시작\n",
      "Epoch 99/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 10.8144 - mse: 10.8144 - mae: 2.3983epoch 끝\n",
      "392/392 [==============================] - 0s 94us/sample - loss: 7.8655 - mse: 7.8655 - mae: 2.0267\n",
      "epoch 시작\n",
      "Epoch 100/100\n",
      " 32/392 [=>............................] - ETA: 0s - loss: 11.9316 - mse: 11.9316 - mae: 2.2634epoch 끝\n",
      "392/392 [==============================] - 0s 92us/sample - loss: 7.9184 - mse: 7.9184 - mae: 2.0600\n"
     ]
    }
   ],
   "source": [
    "model_mm = create_model()\n",
    "hh = model_mm.fit(X, mpg_pd[0].values, epochs=100, callbacks=[MyCallback()]) # 클래스는 인스턴스해서 사용한다.\n",
    "                                                                             # callbacks -> 복수형은 list로 묶어서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation loss를 관찰하다가 loss에 변화가 거의 없으면 학습을 중단시킨다.\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  def on_epoch_end(self, epoch, logs=None):\n",
      "    current = self.get_monitor_value(logs)\n",
      "    if current is None:\n",
      "      return\n",
      "    if self.monitor_op(current - self.min_delta, self.best):\n",
      "      self.best = current\n",
      "      self.wait = 0\n",
      "      if self.restore_best_weights:\n",
      "        self.best_weights = self.model.get_weights()\n",
      "    else:\n",
      "      self.wait += 1\n",
      "      if self.wait >= self.patience:\n",
      "        self.stopped_epoch = epoch\n",
      "        self.model.stop_training = True\n",
      "        if self.restore_best_weights:\n",
      "          if self.verbose > 0:\n",
      "            print('Restoring model weights from the end of the best epoch.')\n",
      "          self.model.set_weights(self.best_weights)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 소스코드를 보고 내가 원하는 기능 만들 때 참조할 수 있다.\n",
    "import inspect\n",
    "print(inspect.getsource(EarlyStopping.on_epoch_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
